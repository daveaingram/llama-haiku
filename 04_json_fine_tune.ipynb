{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an LLM with custom data\n",
    "\n",
    "In this notebook we will prepare a training dataset and use it to train our custom model.\n",
    "\n",
    "You must have `HUGGINGFACE_API_KEY` in a `.env` file for this to work.\n",
    "\n",
    "First, we'll download a training dataset (only needs to run the first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "dataset = \"statworx/haiku\"\n",
    "headers = {\"Authorization\": f\"Bearer {os.environ.get('HUGGINGFACE_API_KEY')}\"}\n",
    "API_URL = f\"https://datasets-server.huggingface.co/parquet?dataset={dataset}\"\n",
    "\n",
    "def query():\n",
    "    response = requests.get(API_URL, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "# get the url to the datafile\n",
    "data = query()\n",
    "url = data[\"parquet_files\"][0][\"url\"]\n",
    "\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "with open('data/haikus.parquet', 'wb') as file:\n",
    "    file.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "Next, we'll load the dataset into a pandas dataframe and prepare a training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Haiku format:\n",
      "I got pregnant, but. / I don't even wanna think. / About what happened?\n",
      "\n",
      "With new-lines:\n",
      "\n",
      "{\"haiku\": [\"I got pregnant, but.\", \"I don't even wanna think.\", \"About what happened?\"]}\n",
      "\n",
      "{\"haiku\": [\"Sunrise at the pier.\", \"Calamari, fishermen.\", \"Bowing to the sea.\"]}\n",
      "\n",
      "{\"haiku\": [\"If she decides to.\", \"Get back together then you.\", \"Need a new sister?\"]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "haikus = pd.read_parquet(\"data/haikus.parquet\")\n",
    "\n",
    "# Let's take just a random sample of 1000 of these.\n",
    "haikus = haikus.sample(5000)\n",
    "\n",
    "# print the first haiku\n",
    "print(\"Original Haiku format:\")\n",
    "print(haikus[\"text\"].iloc[0])\n",
    "\n",
    "# Format as JSON\n",
    "def json_formatter(haiku):\n",
    "    haiku_list = haiku.split(\" / \")\n",
    "    haiku_json = json.dumps({\n",
    "        \"haiku\": haiku_list\n",
    "    })\n",
    "    return haiku_json\n",
    "\n",
    "haikus[\"text\"] = haikus[\"text\"].apply(json_formatter)\n",
    "\n",
    "print(\"\\nWith new-lines:\")\n",
    "# Look at 3 of them\n",
    "for i in range(3):\n",
    "    print(\"\\n\" + haikus[\"text\"].iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good enough, let's train!\n",
    "\n",
    "Next up we'll use the \"text\" and \"keywords\" columns to construct a training dataset to use for training our model with this filtered set of haikus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting generation max_new_tokens to 16384 to correspond with the max sequence length assigned to the output feature or the global max sequence length. This will ensure that the correct number of tokens are generated at inference time. To override this behavior, set `generation.max_new_tokens` to a different value in your Ludwig config.\n",
      "\n",
      "╒════════════════════════╕\n",
      "│ EXPERIMENT DESCRIPTION │\n",
      "╘════════════════════════╛\n",
      "\n",
      "╒══════════════════╤══════════════════════════════════════════════════════════════════════════════════════╕\n",
      "│ Experiment name  │ api_experiment                                                                       │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Model name       │ run                                                                                  │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Output directory │ /home/dave/code/llama-haiku/results/api_experiment_run_5                             │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ ludwig_version   │ '0.9.1'                                                                              │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ command          │ ('/home/dave/miniconda3/envs/llm/lib/python3.9/site-packages/ipykernel_launcher.py ' │\n",
      "│                  │  '--f=/home/dave/.local/share/jupyter/runtime/kernel-v2-3028XxLw5jLBn0uU.json')      │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ commit_hash      │ 'cb7433c8feae'                                                                       │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ random_seed      │ 42                                                                                   │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ data_format      │ \"<class 'pandas.core.frame.DataFrame'>\"                                              │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ torch_version    │ '2.1.2+cu121'                                                                        │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ compute          │ {   'arch_list': [   'sm_50',                                                        │\n",
      "│                  │                      'sm_60',                                                        │\n",
      "│                  │                      'sm_70',                                                        │\n",
      "│                  │                      'sm_75',                                                        │\n",
      "│                  │                      'sm_80',                                                        │\n",
      "│                  │                      'sm_86',                                                        │\n",
      "│                  │                      'sm_90'],                                                       │\n",
      "│                  │     'devices': {   0: {   'device_capability': (8, 9),                               │\n",
      "│                  │                           'device_properties': \"_CudaDeviceProperties(name='NVIDIA \" │\n",
      "│                  │                                                \"GeForce RTX 4090', major=8, \"        │\n",
      "│                  │                                                'minor=9, total_memory=24563MB, '     │\n",
      "│                  │                                                'multi_processor_count=128)',         │\n",
      "│                  │                           'gpu_type': 'NVIDIA GeForce RTX 4090'}},                   │\n",
      "│                  │     'gencode_flags': '-gencode compute=compute_50,code=sm_50 -gencode '              │\n",
      "│                  │                      'compute=compute_60,code=sm_60 -gencode '                       │\n",
      "│                  │                      'compute=compute_70,code=sm_70 -gencode '                       │\n",
      "│                  │                      'compute=compute_75,code=sm_75 -gencode '                       │\n",
      "│                  │                      'compute=compute_80,code=sm_80 -gencode '                       │\n",
      "│                  │                      'compute=compute_86,code=sm_86 -gencode '                       │\n",
      "│                  │                      'compute=compute_90,code=sm_90',                                │\n",
      "│                  │     'gpus_per_node': 1,                                                              │\n",
      "│                  │     'num_nodes': 1}                                                                  │\n",
      "╘══════════════════╧══════════════════════════════════════════════════════════════════════════════════════╛\n",
      "\n",
      "╒═══════════════╕\n",
      "│ LUDWIG CONFIG │\n",
      "╘═══════════════╛\n",
      "\n",
      "User-specified config (with upgrades):\n",
      "\n",
      "{   'adapter': {'type': 'lora'},\n",
      "    'backend': {'type': 'local'},\n",
      "    'base_model': 'HuggingFaceH4/zephyr-7b-beta',\n",
      "    'input_features': [{'name': 'keywords', 'type': 'text'}],\n",
      "    'ludwig_version': '0.9.1',\n",
      "    'model_type': 'llm',\n",
      "    'output_features': [{'name': 'text', 'type': 'text'}],\n",
      "    'quantization': {'bits': 4},\n",
      "    'trainer': {   'batch_size': 2,\n",
      "                   'epochs': 5,\n",
      "                   'gradient_accumulation_steps': 8,\n",
      "                   'learning_rate': 0.0003,\n",
      "                   'learning_rate_scheduler': {'warmup_fraction': 0.01},\n",
      "                   'type': 'finetune'}}\n",
      "\n",
      "Full config saved to:\n",
      "/home/dave/code/llama-haiku/results/api_experiment_run_5/api_experiment/model/model_hyperparameters.json\n",
      "\n",
      "╒═══════════════╕\n",
      "│ PREPROCESSING │\n",
      "╘═══════════════╛\n",
      "\n",
      "No cached dataset found at /home/dave/code/llama-haiku/5eb9de02b0c511eea60a01fcb3ec9195.training.hdf5. Preprocessing the dataset.\n",
      "Using full dataframe\n",
      "Building dataset (it may take a while)\n",
      "Loaded HuggingFace implementation of HuggingFaceH4/zephyr-7b-beta tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of feature 'keywords': 16 (without start and stop symbols)\n",
      "Max sequence length is 16 for feature 'keywords'\n",
      "Loaded HuggingFace implementation of HuggingFaceH4/zephyr-7b-beta tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of feature 'text': 37 (without start and stop symbols)\n",
      "Max sequence length is 37 for feature 'text'\n",
      "Loaded HuggingFace implementation of HuggingFaceH4/zephyr-7b-beta tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of HuggingFaceH4/zephyr-7b-beta tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset: DONE\n",
      "Writing preprocessed training set cache to /home/dave/code/llama-haiku/5eb9de02b0c511eea60a01fcb3ec9195.training.hdf5\n",
      "Writing preprocessed validation set cache to /home/dave/code/llama-haiku/5eb9de02b0c511eea60a01fcb3ec9195.validation.hdf5\n",
      "Writing preprocessed test set cache to /home/dave/code/llama-haiku/5eb9de02b0c511eea60a01fcb3ec9195.test.hdf5\n",
      "Writing train set metadata to /home/dave/code/llama-haiku/5eb9de02b0c511eea60a01fcb3ec9195.meta.json\n",
      "\n",
      "Dataset Statistics\n",
      "╒════════════╤═══════════════╤════════════════════╕\n",
      "│ Dataset    │   Size (Rows) │ Size (In Memory)   │\n",
      "╞════════════╪═══════════════╪════════════════════╡\n",
      "│ Training   │          3500 │ 820.44 Kb          │\n",
      "├────────────┼───────────────┼────────────────────┤\n",
      "│ Validation │           500 │ 117.31 Kb          │\n",
      "├────────────┼───────────────┼────────────────────┤\n",
      "│ Test       │          1000 │ 234.50 Kb          │\n",
      "╘════════════╧═══════════════╧════════════════════╛\n",
      "\n",
      "╒═══════╕\n",
      "│ MODEL │\n",
      "╘═══════╛\n",
      "\n",
      "Warnings and other logs:\n",
      "Loading large language model...\n",
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:12<00:00,  1.62s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Loaded HuggingFace implementation of HuggingFaceH4/zephyr-7b-beta tokenizer\n",
      "==================================================\n",
      "Trainable Parameter Summary For Fine-Tuning\n",
      "Fine-tuning with adapter: lora\n",
      "trainable params: 3,407,872 || all params: 7,245,139,968 || trainable%: 0.04703666202518836\n",
      "==================================================\n",
      "\n",
      "╒══════════╕\n",
      "│ TRAINING │\n",
      "╘══════════╛\n",
      "\n",
      "Creating fresh model training run.\n",
      "Training for 8750 step(s), approximately 5 epoch(s).\n",
      "Early stopping policy: 5 round(s) of evaluation, or 8750 step(s), approximately 5 epoch(s).\n",
      "\n",
      "Starting with step 0, epoch: 0\n",
      "Training:  20%|██        | 1750/8750 [04:00<16:23,  7.12it/s, loss=0.231]\n",
      "Running evaluation for step: 1750, epoch: 1\n",
      "Evaluation valid: 100%|██████████| 250/250 [00:17<00:00, 14.38it/s]\n",
      "Input: tonight\n",
      "Output: < {\"haiku\": [\"I'm like you.\", \"B to tonight I.\", \"I's in me.\".\"]}\n",
      "--------------------\n",
      "Input: yard wild\n",
      "Output: < {\"haiku\": [\"Wng man.\", \"Inildling in the yard.\", \"Wildflowasparb.\"]}\n",
      "--------------------\n",
      "Input: sapling\n",
      "Output: {\" {\" {\" {\" {\"haiku\": [\"Woonlit.\", \"A the sapling.\",s leaves.\", \"A o bird.\"]}\n",
      "--------------------\n",
      "Input: swirls\n",
      "Output: < {\"haiku\": [\"Ainteryirls.\", \"A the garden home.\",tyard.\", \"A butter leaves leaves.\"]}\n",
      "--------------------\n",
      "Input: tripod or\n",
      "Output: < {\"haiku\": [\"I' so nice.\", \"For a tripod or a.\",.\", \"A ground of a pole.\"]}\n",
      "--------------------\n",
      "Evaluation test : 100%|██████████| 500/500 [00:35<00:00, 14.16it/s]\n",
      "Input: allergies are\n",
      "Output: {\" {\" {\" {\" {\" {\"haiku\": [\"I allergies are.\", \"Sorying to kill me,.\", \"I can sick live her.\"]}\n",
      "--------------------\n",
      "Input: pennies late\n",
      "Output: < {\"haiku\": [\"Iusky the the \"Theust's old of p pennies.\", \"Late afternoon.\".\"]}\n",
      "--------------------\n",
      "Input: spider\n",
      "Output: < {\"haiku\": [\"A a a spider.\", \"In I I disappeared.\", but?\", \"I to my face.\"]}\n",
      "--------------------\n",
      "Input: see us\n",
      "Output: {\" {\" {\" {\" {\" {\"haiku\": [\"I suniteens.\", \"See't see see us.\", \"In rain.\"]}\n",
      "--------------------\n",
      "Input: would\n",
      "Output: < {\"haiku\": [\"I, I, I.\",.\", \"Love to see what about.\",.\",\", \"You' for me,.\"]}\n",
      "--------------------\n",
      "Evaluation took 53.5635s\n",
      "\n",
      "╒═══════════════════════╤════════════╤══════════════╤════════════╕\n",
      "│                       │      train │   validation │       test │\n",
      "╞═══════════════════════╪════════════╪══════════════╪════════════╡\n",
      "│ bleu                  │     0.0697 │       0.0635 │     0.0774 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ char_error_rate       │     0.4782 │       0.4744 │     0.4708 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ loss                  │     1.8217 │       1.6087 │     1.5931 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ next_token_perplexity │ 17906.0684 │   17190.2754 │ 17164.3066 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ perplexity            │ 31980.9062 │   31967.0898 │ 31973.3125 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_fmeasure       │     0.4466 │       0.4432 │     0.4564 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_precision      │     0.4639 │       0.4604 │     0.4736 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_recall         │     0.4333 │       0.4294 │     0.4426 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_fmeasure       │     0.1591 │       0.1514 │     0.1644 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_precision      │     0.1654 │       0.1575 │     0.1709 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_recall         │     0.1543 │       0.1465 │     0.1592 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_fmeasure       │     0.4295 │       0.4288 │     0.4403 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_precision      │     0.4462 │       0.4452 │     0.4567 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_recall         │     0.4167 │       0.4156 │     0.4271 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_fmeasure    │     0.4443 │       0.4406 │     0.4535 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_precision   │     0.4616 │       0.4577 │     0.4706 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_recall      │     0.4311 │       0.4270 │     0.4400 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ sequence_accuracy     │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ token_accuracy        │     0.0002 │       0.0003 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ word_error_rate       │     0.8680 │       0.8853 │     0.8816 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ combined_loss         │     1.8217 │       1.6087 │     1.5931 │\n",
      "╘═══════════════════════╧════════════╧══════════════╧════════════╛\n",
      "Evaluation validation metric: 'text' 'loss' improved.\n",
      "'text' 'loss' decreased by inf.\n",
      "New best model saved.\n",
      "\n",
      "Training:  40%|████      | 3500/8750 [08:50<12:34,  6.96it/s, loss=0.129]   \n",
      "Running evaluation for step: 3500, epoch: 2\n",
      "Evaluation valid: 100%|██████████| 250/250 [00:17<00:00, 14.23it/s]\n",
      "Input: tonight\n",
      "Output: < {\"haiku\": [\"I'm like you.\", \"A around, I.\", \"I's playing it home.\"]}\n",
      "--------------------\n",
      "Input: yard wild\n",
      "Output: < {\"haiku\": [\"W',.\", \"Playildling in the yard.\", \"Wild,asparb.\"]}\n",
      "--------------------\n",
      "Input: sapling\n",
      "Output: {\" {\"haiku\": [\"Aoonlit.\", \"A the sapling.\",s shadow.\", \"A o leaf.\"]}\n",
      "--------------------\n",
      "Input: swirls\n",
      "Output: < {\"haiku\": [\"Theinteryirls.\", \"The the empty home.\",tyard.\", \"A dead leaves leaves.\"]}\n",
      "--------------------\n",
      "Input: tripod or\n",
      "Output: < {\"haiku\": [\"I' not useful.\", \"For a tripod or a.\",.\", \"A wall of a car.\"]}\n",
      "--------------------\n",
      "Evaluation test : 100%|██████████| 500/500 [00:35<00:00, 14.05it/s]\n",
      "Input: allergies are\n",
      "Output: {\" {\"haiku\": [\"All allergies are.\", \"Krying to kill me,.\", \"My can to my my.\"]}\n",
      "--------------------\n",
      "Input: pennies late\n",
      "Output: < {\"haiku\": [\"Preamy the.\", \"Thero's old of p pennies.\", \"Late in.\".\"]}\n",
      "--------------------\n",
      "Input: spider\n",
      "Output: < {\"haiku\": [\"Sp a a spider.\", \"The I I disappeared.\", I?\", \"In to my face.\"]}\n",
      "--------------------\n",
      "Input: see us\n",
      "Output: {\" {\"haiku\": [\"We wayiteens.\", \"See't see see us.\", \"But rain.\"]}\n",
      "--------------------\n",
      "Input: would\n",
      "Output: < {\"haiku\": [\"I, I, I.\",.\", \"Love to see what about.\",?\", \"You' for me more.\"]}\n",
      "--------------------\n",
      "Evaluation took 54.5246s\n",
      "\n",
      "╒═══════════════════════╤════════════╤══════════════╤════════════╕\n",
      "│                       │      train │   validation │       test │\n",
      "╞═══════════════════════╪════════════╪══════════════╪════════════╡\n",
      "│ bleu                  │     0.0817 │       0.0721 │     0.0955 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ char_error_rate       │     0.4555 │       0.4091 │     0.4004 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ loss                  │     1.4878 │       1.6109 │     1.5822 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ next_token_perplexity │ 16945.6973 │   17206.7266 │ 17155.8301 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ perplexity            │ 31873.0273 │   31897.0195 │ 31902.1289 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_fmeasure       │     0.4826 │       0.4436 │     0.4592 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_precision      │     0.4975 │       0.4585 │     0.4752 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_recall         │     0.4708 │       0.4318 │     0.4465 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_fmeasure       │     0.1910 │       0.1524 │     0.1697 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_precision      │     0.1974 │       0.1576 │     0.1757 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_recall         │     0.1859 │       0.1483 │     0.1650 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_fmeasure       │     0.4685 │       0.4301 │     0.4453 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_precision      │     0.4829 │       0.4445 │     0.4605 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_recall         │     0.4571 │       0.4186 │     0.4331 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_fmeasure    │     0.4811 │       0.4408 │     0.4565 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_precision   │     0.4960 │       0.4555 │     0.4723 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_recall      │     0.4694 │       0.4290 │     0.4439 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ sequence_accuracy     │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ token_accuracy        │     0.0001 │       0.0003 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ word_error_rate       │     0.8465 │       0.7375 │     0.7247 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ combined_loss         │     1.4878 │       1.6109 │     1.5822 │\n",
      "╘═══════════════════════╧════════════╧══════════════╧════════════╛\n",
      "Last improvement of text validation loss happened 1750 step(s) ago.\n",
      "\n",
      "Training:  60%|██████    | 5250/8750 [13:38<08:13,  7.09it/s, loss=0.117]   \n",
      "Running evaluation for step: 5250, epoch: 3\n",
      "Evaluation valid: 100%|██████████| 250/250 [00:18<00:00, 13.84it/s]\n",
      "Input: tonight\n",
      "Output: < {\"haiku\": [\"I'm rather you.\", \"A around tonight I.\", \"I's pitch it home.\"]}\n",
      "--------------------\n",
      "Input: yard wild\n",
      "Output: < {\"haiku\": [\"W' people.\", \"Playildling in the yard.\", \"Wild,asparb.\"]}\n",
      "--------------------\n",
      "Input: sapling\n",
      "Output: {\" {\"haiku\": [\"Aorshadow.\", \"A the sapling.\",s shadow.\", \"A o leaf.\"]}\n",
      "--------------------\n",
      "Input: swirls\n",
      "Output: < {\"haiku\": [\"Swinteryirls.\", \"The the corner home halltyard.\", \"A child last leaves.\"]}\n",
      "--------------------\n",
      "Input: tripod or\n",
      "Output: < {\"haiku\": [\"I' all useful.\", \"For a tripod or mon.\",.\", \"A camera of the camera.\"]}\n",
      "--------------------\n",
      "Evaluation test : 100%|██████████| 500/500 [00:35<00:00, 13.95it/s]\n",
      "Input: allergies are\n",
      "Output: {\"ies {\"haiku\": [\"All allergies are.\", \"Sorying to ruin me,.\", \"My can to my my.\"]}\n",
      "--------------------\n",
      "Input: pennies late\n",
      "Output: < {\"haiku\": [\"Proing the.\", \"Thero's p of p pennies.\", \"Late spring night.\"]}\n",
      "--------------------\n",
      "Input: spider\n",
      "Output: < {\"haiku\": [\"Sp as a spider.\", \"The I I disappeared.\", I?\", \"I to me head.\"]}\n",
      "--------------------\n",
      "Input: see us\n",
      "Output: {\" {\"haiku\": [\"I wayittens.\", \"Can't see see us.\", \"In night.\"]}\n",
      "--------------------\n",
      "Input: would\n",
      "Output: < {\"haiku\": [\"I, I, I'.\", \"Like to see what about your?\", \"You' for me more.\"]}\n",
      "--------------------\n",
      "Evaluation took 54.7126s\n",
      "\n",
      "╒═══════════════════════╤════════════╤══════════════╤════════════╕\n",
      "│                       │      train │   validation │       test │\n",
      "╞═══════════════════════╪════════════╪══════════════╪════════════╡\n",
      "│ bleu                  │     0.1168 │       0.0782 │     0.0953 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ char_error_rate       │     0.4006 │       0.4109 │     0.4063 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ loss                  │     1.3263 │       1.6562 │     1.6239 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ next_token_perplexity │ 16469.4824 │   17125.4121 │ 17056.5176 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ perplexity            │ 31759.7910 │   31830.9570 │ 31831.9883 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_fmeasure       │     0.5071 │       0.4395 │     0.4505 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_precision      │     0.5206 │       0.4474 │     0.4579 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_recall         │     0.4962 │       0.4336 │     0.4453 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_fmeasure       │     0.2129 │       0.1556 │     0.1653 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_precision      │     0.2190 │       0.1583 │     0.1681 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_recall         │     0.2080 │       0.1536 │     0.1633 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_fmeasure       │     0.4964 │       0.4272 │     0.4362 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_precision      │     0.5095 │       0.4349 │     0.4433 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_recall         │     0.4858 │       0.4215 │     0.4313 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_fmeasure    │     0.5053 │       0.4378 │     0.4474 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_precision   │     0.5187 │       0.4457 │     0.4548 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_recall      │     0.4944 │       0.4319 │     0.4424 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ sequence_accuracy     │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ token_accuracy        │     0.0002 │       0.0001 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ word_error_rate       │     0.7583 │       0.7325 │     0.7321 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ combined_loss         │     1.3263 │       1.6562 │     1.6239 │\n",
      "╘═══════════════════════╧════════════╧══════════════╧════════════╛\n",
      "Last improvement of text validation loss happened 3500 step(s) ago.\n",
      "\n",
      "Training:  80%|████████  | 7000/8750 [18:28<04:01,  7.24it/s, loss=0.156]   \n",
      "Running evaluation for step: 7000, epoch: 4\n",
      "Evaluation valid: 100%|██████████| 250/250 [00:17<00:00, 14.35it/s]\n",
      "Input: tonight\n",
      "Output: < {\"haiku\": [\"I'm really you.\", \"A anywhere he everywhere.\", \"I's pitch it home.\"]}\n",
      "--------------------\n",
      "Input: yard wild\n",
      "Output: < {\"haiku\": [\"W' map.\", \"Playildling in the yard.\", \"Wild,asparb.\"]}\n",
      "--------------------\n",
      "Input: sapling\n",
      "Output: {\" {\" {\" {\" {\"haiku\": [\"Aorlit.\", \"A the sapling.\",s leaves.\", \"A o leaf.\"]}\n",
      "--------------------\n",
      "Input: swirls\n",
      "Output: < {\"haiku\": [\"Swintersirls.\", \"The the valley home courtyard.\", \"A child white leaves.\"]}\n",
      "--------------------\n",
      "Input: tripod or\n",
      "Output: < {\"haiku\": [\"I' a useful.\", \"For a tripod or mon lens.\", \"A camera of the camera.\"]}\n",
      "--------------------\n",
      "Evaluation test : 100%|██████████| 500/500 [00:35<00:00, 14.26it/s]\n",
      "Input: allergies are\n",
      "Output: {\" {\" {\" {\" {\"ies {\"haiku\": [\"All allergies are.\", \"Sorying to ruin me,? \"The can to graduate Jesus.\"]}\n",
      "--------------------\n",
      "Input: pennies late\n",
      "Output: < {\"haiku\": [\"Pandy the the \"Aro's p of p pennies.\", \"Late in light.\"]}\n",
      "--------------------\n",
      "Input: spider\n",
      "Output: < {\"haiku\": [\"A before a spider.\", \"The I I disappeared.\", oh?\", \"I to my head.\"]}\n",
      "--------------------\n",
      "Input: see us\n",
      "Output: {\" {\" {\" {\" {\" {\"haiku\": [\"We wayittens.\", \"See't see see us.\", \"In night.\"]}\n",
      "--------------------\n",
      "Input: would\n",
      "Output: < {\"haiku\": [\"I, I, I'.\", \"Like to see what about.\",?\", \"You', me more.\"]}\n",
      "--------------------\n",
      "Evaluation took 53.2543s\n",
      "\n",
      "╒═══════════════════════╤════════════╤══════════════╤════════════╕\n",
      "│                       │      train │   validation │       test │\n",
      "╞═══════════════════════╪════════════╪══════════════╪════════════╡\n",
      "│ bleu                  │     0.1760 │       0.0587 │     0.0748 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ char_error_rate       │     0.3259 │       0.4842 │     0.4761 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ loss                  │     1.0573 │       1.7657 │     1.7280 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ next_token_perplexity │ 15696.3418 │   16965.9375 │ 16873.7383 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ perplexity            │ 31673.0430 │   31333.7578 │ 31332.4727 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_fmeasure       │     0.5693 │       0.4273 │     0.4420 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_precision      │     0.5801 │       0.4387 │     0.4524 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_recall         │     0.5604 │       0.4184 │     0.4341 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_fmeasure       │     0.2857 │       0.1407 │     0.1550 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_precision      │     0.2913 │       0.1444 │     0.1589 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_recall         │     0.2810 │       0.1378 │     0.1520 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_fmeasure       │     0.5620 │       0.4130 │     0.4261 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_precision      │     0.5725 │       0.4240 │     0.4361 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_recall         │     0.5533 │       0.4045 │     0.4186 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_fmeasure    │     0.5683 │       0.4261 │     0.4397 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_precision   │     0.5790 │       0.4374 │     0.4501 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_recall      │     0.5594 │       0.4172 │     0.4319 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ sequence_accuracy     │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ token_accuracy        │     0.0002 │       0.0002 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ word_error_rate       │     0.6324 │       0.9042 │     0.8905 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ combined_loss         │     1.0573 │       1.7657 │     1.7280 │\n",
      "╘═══════════════════════╧════════════╧══════════════╧════════════╛\n",
      "Last improvement of text validation loss happened 5250 step(s) ago.\n",
      "\n",
      "Training: 100%|██████████| 8750/8750 [23:14<00:00,  7.55it/s, loss=0.0899]  \n",
      "Running evaluation for step: 8750, epoch: 5\n",
      "Evaluation valid: 100%|██████████| 250/250 [00:17<00:00, 14.58it/s]\n",
      "Input: tonight\n",
      "Output: < {\"haiku\": [\"I'm like you.\", \"B anywhere, I.\", \"We's pitch us.\".\"]}\n",
      "--------------------\n",
      "Input: yard wild\n",
      "Output: < {\"haiku\": [\"W' love.\", \"Playildle in the yard.\", \"Wild,asparb.\"]}\n",
      "--------------------\n",
      "Input: sapling\n",
      "Output: < {\"haiku\": [\"Aorlit.\", \"A the riverling.\",s branches.\", \"A o bird.\"]}\n",
      "--------------------\n",
      "Input: swirls\n",
      "Output: < {\"haiku\": [\"Swinterblirls.\", \"The the dry home courtyard.\", \"Le leaf last leaves.\"]}\n",
      "--------------------\n",
      "Input: tripod or\n",
      "Output: < {\"haiku\": [\"I should all useful.\", \"For a tripod or a.\",.\", \"A ground of the car.\"]}\n",
      "--------------------\n",
      "Evaluation test : 100%|██████████| 500/500 [00:35<00:00, 14.28it/s]\n",
      "Input: allergies are\n",
      "Output: {\"ies {\"haiku\": [\"My allergies are.\", \"Actrying me ruin me,.\", \"My even my finish him.\"]}\n",
      "--------------------\n",
      "Input: pennies late\n",
      "Output: < {\"haiku\": [\"Iimey the.\", \"Aimes's p of p pennies.\", \"Late in.\".\"]}\n",
      "--------------------\n",
      "Input: spider\n",
      "Output: < {\"haiku\": [\"M before a spider.\", \"The I I disappeared.\", so?\", \"In to my face.\"]}\n",
      "--------------------\n",
      "Input: see us\n",
      "Output: {\" {\"haiku\": [\"We wayittens.\", \"See't yet see us.\", \"Spring night.\"]}\n",
      "--------------------\n",
      "Input: would\n",
      "Output: < {\"haiku\": [\"I, I, what'.\", \"Like to see what about.\",?\", \"You', me more.\"]}\n",
      "--------------------\n",
      "Evaluation took 52.9241s\n",
      "\n",
      "╒═══════════════════════╤════════════╤══════════════╤════════════╕\n",
      "│                       │      train │   validation │       test │\n",
      "╞═══════════════════════╪════════════╪══════════════╪════════════╡\n",
      "│ bleu                  │     0.2487 │       0.0641 │     0.0881 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ char_error_rate       │     0.2844 │       0.4241 │     0.4156 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ loss                  │     0.8125 │       1.9207 │     1.8752 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ next_token_perplexity │ 14957.1123 │   16907.9414 │ 16812.3359 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ perplexity            │ 31735.1445 │   31842.2207 │ 31854.1270 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_fmeasure       │     0.6486 │       0.4166 │     0.4270 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_precision      │     0.6551 │       0.4264 │     0.4360 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_recall         │     0.6434 │       0.4093 │     0.4204 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_fmeasure       │     0.3736 │       0.1314 │     0.1462 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_precision      │     0.3771 │       0.1346 │     0.1493 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_recall         │     0.3710 │       0.1291 │     0.1440 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_fmeasure       │     0.6429 │       0.4012 │     0.4125 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_precision      │     0.6493 │       0.4106 │     0.4210 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_recall         │     0.6379 │       0.3943 │     0.4061 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_fmeasure    │     0.6476 │       0.4145 │     0.4242 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_precision   │     0.6541 │       0.4242 │     0.4331 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_recall      │     0.6425 │       0.4073 │     0.4177 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ sequence_accuracy     │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ token_accuracy        │     0.0001 │       0.0002 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ word_error_rate       │     0.5727 │       0.7675 │     0.7503 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ combined_loss         │     0.8125 │       1.9207 │     1.8752 │\n",
      "╘═══════════════════════╧════════════╧══════════════╧════════════╛\n",
      "Last improvement of text validation loss happened 7000 step(s) ago.\n",
      "\n",
      "Training: 100%|██████████| 8750/8750 [24:07<00:00,  6.04it/s, loss=0.0899]\n",
      "\n",
      "╒═════════════════╕\n",
      "│ TRAINING REPORT │\n",
      "╘═════════════════╛\n",
      "\n",
      "╒══════════════════════════════╤════════════════════╕\n",
      "│ Validation feature           │ text               │\n",
      "├──────────────────────────────┼────────────────────┤\n",
      "│ Validation metric            │ loss               │\n",
      "├──────────────────────────────┼────────────────────┤\n",
      "│ Best model step              │ 1750               │\n",
      "├──────────────────────────────┼────────────────────┤\n",
      "│ Best model epoch             │ 2                  │\n",
      "├──────────────────────────────┼────────────────────┤\n",
      "│ Best model's validation loss │ 1.60871422290802   │\n",
      "├──────────────────────────────┼────────────────────┤\n",
      "│ Best model's test loss       │ 1.5930792093276978 │\n",
      "╘══════════════════════════════╧════════════════════╛\n",
      "\n",
      "Finished: api_experiment_run\n",
      "Saved to: /home/dave/code/llama-haiku/results/api_experiment_run_5\n",
      "\n",
      "╒══════════╕\n",
      "│ FINISHED │\n",
      "╘══════════╛\n",
      "\n",
      "contents of output directory: /home/dave/code/llama-haiku/results/api_experiment_run_5\n",
      "\t description.json\n",
      "\t model\n",
      "\t training_statistics.json\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import yaml\n",
    "\n",
    "from ludwig.api import LudwigModel\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<|system|>\n",
    "You are a haiku writer and respond to all questions with a colorful, poetic haiku\n",
    "You always output the Haiku as JSON, where there is a key called \"haiku\" and that is an array of 3 lines of the Haiku.</s>\n",
    "<|user|>\n",
    "Please write me a haiku about {keywords}. Write this as JSON.</s>\n",
    "<|assistant|>\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# Build out the configuration\n",
    "config = yaml.safe_load(\n",
    "    \"\"\"\n",
    "model_type: llm\n",
    "base_model: HuggingFaceH4/zephyr-7b-beta\n",
    "\n",
    "quantization:\n",
    "  bits: 4\n",
    "\n",
    "adapter:\n",
    "  type: lora\n",
    "\n",
    "input_features:\n",
    "  - name: keywords\n",
    "    type: text\n",
    "\n",
    "output_features:\n",
    "  - name: text\n",
    "    type: text\n",
    "\n",
    "trainer:\n",
    "    type: finetune\n",
    "    learning_rate: 0.0003\n",
    "    batch_size: 2\n",
    "    gradient_accumulation_steps: 8\n",
    "    epochs: 5\n",
    "    learning_rate_scheduler:\n",
    "      warmup_fraction: 0.01\n",
    "\n",
    "backend:\n",
    "  type: local\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Define Ludwig model object that drive model training\n",
    "model = LudwigModel(config=config, logging_level=logging.INFO)\n",
    "\n",
    "# initiate model training\n",
    "(\n",
    "    train_stats,  # dictionary containing training statistics\n",
    "    preprocessed_data,  # tuple Ludwig Dataset objects of pre-processed training data\n",
    "    output_directory,  # location of training results stored on disk\n",
    ") = model.train(\n",
    "    dataset=haikus\n",
    ")\n",
    "\n",
    "# list contents of output directory\n",
    "print(\"contents of output directory:\", output_directory)\n",
    "for item in os.listdir(output_directory):\n",
    "    print(\"\\t\", item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, so how'd we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of HuggingFaceH4/zephyr-7b-beta tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 100%|██████████| 1/1 [00:30<00:00, 30.37s/it]\n",
      "Loaded HuggingFace implementation of HuggingFaceH4/zephyr-7b-beta tokenizer\n",
      "Finished predicting in: 35.23s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dave/miniconda3/envs/llm/lib/python3.9/site-packages/ludwig/features/feature_utils.py:102: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(np.log(sequence_probabilities))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                    text_predictions  \\\n",
       " 0  [, {\", ha, iku, \":, [\", W, inter, sol, st, ice...   \n",
       " 1  [, {\", ha, iku, \":, [\", W, inter, sol, st, ice...   \n",
       " 2  [, {\", ha, iku, \":, [\", A, few, flowers, .\",, ...   \n",
       " 3  [, {\", ha, iku, \":, [\", T, ru, cks, on, the, h...   \n",
       " 4  [, {\", ha, iku, \":, [\", Data, science, gone, w...   \n",
       " \n",
       "                                   text_probabilities  \\\n",
       " 0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       " 1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       " 2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       " 3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       " 4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       " \n",
       "                                        text_response  text_probability  \n",
       " 0  [{\"haiku\": [\"Winter solstice.\", \"The icicles o...              -inf  \n",
       " 1  [{\"haiku\": [\"Winter solstice.\", \"The trees, st...              -inf  \n",
       " 2  [{\"haiku\": [\"A few flowers.\", \"On the sidewalk...              -inf  \n",
       " 3  [{\"haiku\": [\"Trucks on the highway.\", \"Their h...              -inf  \n",
       " 4  [{\"haiku\": [\"Data science gone wrong.\", \"I'm n...              -inf  ,\n",
       " 'results')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict({\n",
    "    \"keywords\": \n",
    "        [\n",
    "            \"icicles\",\n",
    "            \"trees\",\n",
    "            \"flowers on the sidewalk\", \n",
    "            \"trucks on the highway\", \n",
    "            \"data science gone wrong\"\n",
    "        ]\n",
    "    })\n",
    "\n",
    "response = model.predict(df)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"haiku\": [\"Winter solstice.\", \"The icicles on the roof.\", \"Grow longer.\"]}\n",
      "\n",
      "{\"haiku\": [\"Winter solstice.\", \"The trees, still standing.\", \"In the wind.\"]}\n",
      "\n",
      "{\"haiku\": [\"A few flowers.\", \"On the sidewalk.\", \"In the rain.\"]}\n",
      "\n",
      "{\"haiku\": [\"Trucks on the highway.\", \"Their headlights like stars.\", \"In the night sky.\"]}\n",
      "\n",
      "{\"haiku\": [\"Data science gone wrong.\", \"I'm not even mad, I'm just.\", \"Laughing at this shit.\"]}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answers = response[0][\"text_response\"]\n",
    "\n",
    "for a in answers:\n",
    "    print(a[0] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, well, we'll leave it at that \n",
    "\n",
    "### Now we'll save the model to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded to `https://huggingface.co/querri/zephyr-haiku-json/tree/main/` with repository name `querri/zephyr-haiku-json`\n"
     ]
    }
   ],
   "source": [
    "!ludwig upload hf_hub -r querri/zephyr-haiku-json -m /home/dave/code/llama-haiku/results/api_experiment_run_5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
