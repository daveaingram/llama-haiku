{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an LLM with custom data\n",
    "\n",
    "In this notebook we will prepare a training dataset and use it to train our custom model.\n",
    "\n",
    "You must have `HUGGINGFACE_API_KEY` in a `.env` file for this to work.\n",
    "\n",
    "First, we'll download a training dataset (only needs to run the first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "dataset = \"statworx/haiku\"\n",
    "headers = {\"Authorization\": f\"Bearer {os.environ.get('HUGGINGFACE_API_KEY')}\"}\n",
    "API_URL = f\"https://datasets-server.huggingface.co/parquet?dataset={dataset}\"\n",
    "\n",
    "def query():\n",
    "    response = requests.get(API_URL, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "# get the url to the datafile\n",
    "data = query()\n",
    "url = data[\"parquet_files\"][0][\"url\"]\n",
    "\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "with open('data/haikus.parquet', 'wb') as file:\n",
    "    file.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "Next, we'll load the dataset into a pandas dataframe and prepare a training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>text_phonemes</th>\n",
       "      <th>keywords</th>\n",
       "      <th>keyword_phonemes</th>\n",
       "      <th>gruen_score</th>\n",
       "      <th>text_punc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>Delicate savage. / You'll never hold the cinde...</td>\n",
       "      <td>deh|lax|kaxt sae|vaxjh / yuwl neh|ver hhowld d...</td>\n",
       "      <td>cinder</td>\n",
       "      <td>sihn|der</td>\n",
       "      <td>0.639071</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>A splash and a cry. / Words pulled from the ri...</td>\n",
       "      <td>ax splaesh aend ax kray / werdz puhld frahm dh...</td>\n",
       "      <td>the riverside</td>\n",
       "      <td>dhax rih|ver|sayd</td>\n",
       "      <td>0.563353</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>Steamy, mist rising. / Rocks receiving downwar...</td>\n",
       "      <td>stiy|miy mihst ray|zaxng / raaks rax|siy|vaxng...</td>\n",
       "      <td>mist rising</td>\n",
       "      <td>mihst ray|zaxng</td>\n",
       "      <td>0.538326</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>You were broken glass. / But I touched you eve...</td>\n",
       "      <td>yuw wer brow|kaxn glaes / baht ay tahcht yuw i...</td>\n",
       "      <td>broken glass</td>\n",
       "      <td>brow|kaxn glaes</td>\n",
       "      <td>0.703446</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>Eyes dance with firelight. / The Moon and I ar...</td>\n",
       "      <td>ayz daens wihdh faxr|layt / dhax muwn aend ay ...</td>\n",
       "      <td>eyes dance</td>\n",
       "      <td>ayz daens</td>\n",
       "      <td>0.830985</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49019</th>\n",
       "      <td>haiku_data_2</td>\n",
       "      <td>Alpine Lake. / Mybreaststrokesshiningarc. / To...</td>\n",
       "      <td>ael|payn leyk mih|brehst|strow|kehsh|hhax|nihn...</td>\n",
       "      <td>toward sunrise</td>\n",
       "      <td>tax|waord sahn|rayz</td>\n",
       "      <td>0.685355</td>\n",
       "      <td>Alpine Lake. Mybreaststrokesshiningarc. Toward...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49020</th>\n",
       "      <td>haiku_data_2</td>\n",
       "      <td>Spruce Woods. / Fireweed filling. / The vacancy.</td>\n",
       "      <td>spruws wuhdz fay|er|wiyd fih|laxng dhax vey|ka...</td>\n",
       "      <td>woods</td>\n",
       "      <td>wuhdz</td>\n",
       "      <td>0.568974</td>\n",
       "      <td>Spruce Woods. Fireweed filling. The vacancy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49021</th>\n",
       "      <td>haiku_data_2</td>\n",
       "      <td>Corrugated sun. / Chilies and laundry. / In ro...</td>\n",
       "      <td>kao|rax|gey|taxd sahn chih|liyz aend laon|driy...</td>\n",
       "      <td>sun chilies</td>\n",
       "      <td>sahn chih|liyz</td>\n",
       "      <td>0.551056</td>\n",
       "      <td>Corrugated sun. Chilies and laundry. In roofto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49022</th>\n",
       "      <td>haiku_data_2</td>\n",
       "      <td>Home from war. / We ease out. / The champagne ...</td>\n",
       "      <td>hhowm frahm waor wiy iyz awt dhax shaxm|peyn k...</td>\n",
       "      <td>home</td>\n",
       "      <td>hhowm</td>\n",
       "      <td>0.697112</td>\n",
       "      <td>Home from war. We ease out. The champagne corks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49023</th>\n",
       "      <td>haiku_data_2</td>\n",
       "      <td>'</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>ehn</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49024 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             source                                               text  \\\n",
       "0           bfbarry  Delicate savage. / You'll never hold the cinde...   \n",
       "1           bfbarry  A splash and a cry. / Words pulled from the ri...   \n",
       "2           bfbarry  Steamy, mist rising. / Rocks receiving downwar...   \n",
       "3           bfbarry  You were broken glass. / But I touched you eve...   \n",
       "4           bfbarry  Eyes dance with firelight. / The Moon and I ar...   \n",
       "...             ...                                                ...   \n",
       "49019  haiku_data_2  Alpine Lake. / Mybreaststrokesshiningarc. / To...   \n",
       "49020  haiku_data_2   Spruce Woods. / Fireweed filling. / The vacancy.   \n",
       "49021  haiku_data_2  Corrugated sun. / Chilies and laundry. / In ro...   \n",
       "49022  haiku_data_2  Home from war. / We ease out. / The champagne ...   \n",
       "49023  haiku_data_2                                                  '   \n",
       "\n",
       "                                           text_phonemes        keywords  \\\n",
       "0      deh|lax|kaxt sae|vaxjh / yuwl neh|ver hhowld d...          cinder   \n",
       "1      ax splaesh aend ax kray / werdz puhld frahm dh...   the riverside   \n",
       "2      stiy|miy mihst ray|zaxng / raaks rax|siy|vaxng...     mist rising   \n",
       "3      yuw wer brow|kaxn glaes / baht ay tahcht yuw i...    broken glass   \n",
       "4      ayz daens wihdh faxr|layt / dhax muwn aend ay ...      eyes dance   \n",
       "...                                                  ...             ...   \n",
       "49019  ael|payn leyk mih|brehst|strow|kehsh|hhax|nihn...  toward sunrise   \n",
       "49020  spruws wuhdz fay|er|wiyd fih|laxng dhax vey|ka...           woods   \n",
       "49021  kao|rax|gey|taxd sahn chih|liyz aend laon|driy...     sun chilies   \n",
       "49022  hhowm frahm waor wiy iyz awt dhax shaxm|peyn k...            home   \n",
       "49023                                                                  N   \n",
       "\n",
       "          keyword_phonemes  gruen_score  \\\n",
       "0                 sihn|der     0.639071   \n",
       "1        dhax rih|ver|sayd     0.563353   \n",
       "2          mihst ray|zaxng     0.538326   \n",
       "3          brow|kaxn glaes     0.703446   \n",
       "4                ayz daens     0.830985   \n",
       "...                    ...          ...   \n",
       "49019  tax|waord sahn|rayz     0.685355   \n",
       "49020                wuhdz     0.568974   \n",
       "49021       sahn chih|liyz     0.551056   \n",
       "49022                hhowm     0.697112   \n",
       "49023                  ehn     0.625000   \n",
       "\n",
       "                                               text_punc  \n",
       "0                                                   None  \n",
       "1                                                   None  \n",
       "2                                                   None  \n",
       "3                                                   None  \n",
       "4                                                   None  \n",
       "...                                                  ...  \n",
       "49019  Alpine Lake. Mybreaststrokesshiningarc. Toward...  \n",
       "49020       Spruce Woods. Fireweed filling. The vacancy.  \n",
       "49021  Corrugated sun. Chilies and laundry. In roofto...  \n",
       "49022   Home from war. We ease out. The champagne corks.  \n",
       "49023                                                  '  \n",
       "\n",
       "[49024 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "haikus = pd.read_parquet(\"data/haikus.parquet\")\n",
    "haikus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Haiku format:\n",
      "She wants him to change. / He wants her the way she was. / Their love fades away.\n",
      "\n",
      "With new-lines:\n",
      "She wants him to change.\n",
      "He wants her the way she was.\n",
      "Their love fades away.\n",
      "\n",
      "The swell.\n",
      "Before the river splits.\n",
      "In another name.\n",
      "\n",
      "Just remembered that.\n",
      "David's pumpkins exists.\n",
      "So that's positive.\n",
      "\n",
      "A growing pressure.\n",
      "My sphincter is pulsating.\n",
      "I need to go, Poop.\n",
      "\n",
      "For anyone that.\n",
      "Can see through the illusions.\n",
      "It's easy to see.\n"
     ]
    }
   ],
   "source": [
    "# Let's take just a random sample of 5000 of these.\n",
    "haikus_text = haikus[\"text\"].sample(100)\n",
    "\n",
    "# print the first haiku\n",
    "print(\"Original Haiku format:\")\n",
    "print(haikus_text.iloc[0])\n",
    "\n",
    "# Add newlines\n",
    "haikus_text = haikus_text.str.replace(\" / \", \"\\n\")\n",
    "print(\"\\nWith new-lines:\")\n",
    "print(haikus_text.iloc[0])\n",
    "\n",
    "# Look at 4 more of them\n",
    "for i in range(1, 5):\n",
    "    print(\"\\n\" + haikus_text.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This time we'll keep them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The heart and vases.\n",
      "It's as if it was a rule.\n",
      "Made to be broken.\n",
      "\n",
      "Thunder rolls over.\n",
      "You can't quell your beating heart.\n",
      "Always who dares wins.\n",
      "\n",
      "Why do teachers feel?\n",
      "The need to assign so much.\n",
      "Work before finals.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See if we can get some consistent formatting\n",
    "haikus[\"text\"] = haikus[\"text\"].str.replace(\" / \", \"\\n\")\n",
    "\n",
    "# NOTE: Random sample just to run faster\n",
    "haikus = haikus.sample(1000)\n",
    "\n",
    "for i in range(3):\n",
    "    print(haikus.iloc[i][\"text\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good enough, let's train!\n",
    "\n",
    "Next up we'll use the \"text\" and \"keywords\" columns to construct a training dataset to use for training our model with this filtered set of haikus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.1.2 available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dave/miniconda3/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting generation max_new_tokens to 16384 to correspond with the max sequence length assigned to the output feature or the global max sequence length. This will ensure that the correct number of tokens are generated at inference time. To override this behavior, set `generation.max_new_tokens` to a different value in your Ludwig config.\n",
      "\n",
      "╒════════════════════════╕\n",
      "│ EXPERIMENT DESCRIPTION │\n",
      "╘════════════════════════╛\n",
      "\n",
      "╒══════════════════╤══════════════════════════════════════════════════════════════════════════════════════╕\n",
      "│ Experiment name  │ api_experiment                                                                       │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Model name       │ run                                                                                  │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Output directory │ /home/dave/code/llama-haiku/results/api_experiment_run_3                             │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ ludwig_version   │ '0.9.1'                                                                              │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ command          │ ('/home/dave/miniconda3/envs/llm/lib/python3.9/site-packages/ipykernel_launcher.py ' │\n",
      "│                  │  '--f=/home/dave/.local/share/jupyter/runtime/kernel-v2-3028dcjyQh6U0oAN.json')      │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ commit_hash      │ 'cb7433c8feae'                                                                       │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ random_seed      │ 42                                                                                   │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ data_format      │ \"<class 'pandas.core.frame.DataFrame'>\"                                              │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ torch_version    │ '2.1.2+cu121'                                                                        │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ compute          │ {   'arch_list': [   'sm_50',                                                        │\n",
      "│                  │                      'sm_60',                                                        │\n",
      "│                  │                      'sm_70',                                                        │\n",
      "│                  │                      'sm_75',                                                        │\n",
      "│                  │                      'sm_80',                                                        │\n",
      "│                  │                      'sm_86',                                                        │\n",
      "│                  │                      'sm_90'],                                                       │\n",
      "│                  │     'devices': {   0: {   'device_capability': (8, 9),                               │\n",
      "│                  │                           'device_properties': \"_CudaDeviceProperties(name='NVIDIA \" │\n",
      "│                  │                                                \"GeForce RTX 4090', major=8, \"        │\n",
      "│                  │                                                'minor=9, total_memory=24563MB, '     │\n",
      "│                  │                                                'multi_processor_count=128)',         │\n",
      "│                  │                           'gpu_type': 'NVIDIA GeForce RTX 4090'}},                   │\n",
      "│                  │     'gencode_flags': '-gencode compute=compute_50,code=sm_50 -gencode '              │\n",
      "│                  │                      'compute=compute_60,code=sm_60 -gencode '                       │\n",
      "│                  │                      'compute=compute_70,code=sm_70 -gencode '                       │\n",
      "│                  │                      'compute=compute_75,code=sm_75 -gencode '                       │\n",
      "│                  │                      'compute=compute_80,code=sm_80 -gencode '                       │\n",
      "│                  │                      'compute=compute_86,code=sm_86 -gencode '                       │\n",
      "│                  │                      'compute=compute_90,code=sm_90',                                │\n",
      "│                  │     'gpus_per_node': 1,                                                              │\n",
      "│                  │     'num_nodes': 1}                                                                  │\n",
      "╘══════════════════╧══════════════════════════════════════════════════════════════════════════════════════╛\n",
      "\n",
      "╒═══════════════╕\n",
      "│ LUDWIG CONFIG │\n",
      "╘═══════════════╛\n",
      "\n",
      "User-specified config (with upgrades):\n",
      "\n",
      "{   'adapter': {'type': 'lora'},\n",
      "    'backend': {'type': 'local'},\n",
      "    'base_model': 'HuggingFaceH4/zephyr-7b-beta',\n",
      "    'input_features': [{'name': 'keywords', 'type': 'text'}],\n",
      "    'ludwig_version': '0.9.1',\n",
      "    'model_type': 'llm',\n",
      "    'output_features': [{'name': 'text', 'type': 'text'}],\n",
      "    'quantization': {'bits': 4},\n",
      "    'trainer': {   'batch_size': 2,\n",
      "                   'epochs': 3,\n",
      "                   'gradient_accumulation_steps': 8,\n",
      "                   'learning_rate': 0.0003,\n",
      "                   'learning_rate_scheduler': {'warmup_fraction': 0.01},\n",
      "                   'type': 'finetune'}}\n",
      "\n",
      "Full config saved to:\n",
      "/home/dave/code/llama-haiku/results/api_experiment_run_3/api_experiment/model/model_hyperparameters.json\n",
      "\n",
      "╒═══════════════╕\n",
      "│ PREPROCESSING │\n",
      "╘═══════════════╛\n",
      "\n",
      "No cached dataset found at /home/dave/code/llama-haiku/856785e4b0c111eea60a01fcb3ec9195.training.hdf5. Preprocessing the dataset.\n",
      "Using full dataframe\n",
      "Building dataset (it may take a while)\n",
      "Loaded HuggingFace implementation of HuggingFaceH4/zephyr-7b-beta tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of feature 'keywords': 9 (without start and stop symbols)\n",
      "Max sequence length is 9 for feature 'keywords'\n",
      "Loaded HuggingFace implementation of HuggingFaceH4/zephyr-7b-beta tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of feature 'text': 31 (without start and stop symbols)\n",
      "Max sequence length is 31 for feature 'text'\n",
      "Loaded HuggingFace implementation of HuggingFaceH4/zephyr-7b-beta tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of HuggingFaceH4/zephyr-7b-beta tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset: DONE\n",
      "Writing preprocessed training set cache to /home/dave/code/llama-haiku/856785e4b0c111eea60a01fcb3ec9195.training.hdf5\n",
      "Writing preprocessed validation set cache to /home/dave/code/llama-haiku/856785e4b0c111eea60a01fcb3ec9195.validation.hdf5\n",
      "Writing preprocessed test set cache to /home/dave/code/llama-haiku/856785e4b0c111eea60a01fcb3ec9195.test.hdf5\n",
      "Writing train set metadata to /home/dave/code/llama-haiku/856785e4b0c111eea60a01fcb3ec9195.meta.json\n",
      "\n",
      "Dataset Statistics\n",
      "╒════════════╤═══════════════╤════════════════════╕\n",
      "│ Dataset    │   Size (Rows) │ Size (In Memory)   │\n",
      "╞════════════╪═══════════════╪════════════════════╡\n",
      "│ Training   │           700 │ 164.19 Kb          │\n",
      "├────────────┼───────────────┼────────────────────┤\n",
      "│ Validation │           100 │ 23.56 Kb           │\n",
      "├────────────┼───────────────┼────────────────────┤\n",
      "│ Test       │           200 │ 47.00 Kb           │\n",
      "╘════════════╧═══════════════╧════════════════════╛\n",
      "\n",
      "╒═══════╕\n",
      "│ MODEL │\n",
      "╘═══════╛\n",
      "\n",
      "Warnings and other logs:\n",
      "Loading large language model...\n",
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:04<00:00,  1.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of HuggingFaceH4/zephyr-7b-beta tokenizer\n",
      "==================================================\n",
      "Trainable Parameter Summary For Fine-Tuning\n",
      "Fine-tuning with adapter: lora\n",
      "trainable params: 3,407,872 || all params: 7,245,139,968 || trainable%: 0.04703666202518836\n",
      "==================================================\n",
      "\n",
      "╒══════════╕\n",
      "│ TRAINING │\n",
      "╘══════════╛\n",
      "\n",
      "Creating fresh model training run.\n",
      "Training for 1050 step(s), approximately 3 epoch(s).\n",
      "Early stopping policy: 5 round(s) of evaluation, or 1750 step(s), approximately 5 epoch(s).\n",
      "\n",
      "Starting with step 0, epoch: 0\n",
      "Training:  33%|███▎      | 350/1050 [00:46<01:39,  7.07it/s, loss=0.279]\n",
      "Running evaluation for step: 350, epoch: 1\n",
      "Evaluation valid: 100%|██████████| 50/50 [00:03<00:00, 15.18it/s]\n",
      "Input: get downed\n",
      "Output: The The do we a\n",
      "A get get to when.\n",
      "Get who get downed.\n",
      "--------------------\n",
      "Input: fight\n",
      "Output: < I give a fight.\n",
      "With someone old person..re.\n",
      "Got nothing to lose.\n",
      "--------------------\n",
      "Input: nails\n",
      "Output: nails's nails.\n",
      "I'ill my\n",
      "The can of nails.\n",
      "--------------------\n",
      "Input: Larry\n",
      "Output: <. <. Larry.ams.\n",
      "Lber and Larry Coll,.b.\n",
      "Drownes, Larry N.\n",
      "--------------------\n",
      "Input: too many\n",
      "Output: I' too many.\n",
      "We same about...\n",
      "To calm up hell up.\n",
      "--------------------\n",
      "Evaluation test : 100%|██████████| 100/100 [00:06<00:00, 15.37it/s]\n",
      "Input: potpourri\n",
      "Output: < A' see..\n",
      "A pot pot research. a.\n",
      "With pot potpourri.\n",
      "--------------------\n",
      "Input: waves\n",
      "Output: I Wing the waves.\n",
      "I manray cat.les down\n",
      "Ineneathide me.\n",
      "--------------------\n",
      "Input: nail\n",
      "Output: The Theairing\n",
      "The nail to aceilles.\n",
      "Toail pol the wall.\n",
      "--------------------\n",
      "Input: never been\n",
      "Output: < The' never been.\n",
      "Toed with a doctor one.\n",
      "I is love my life.\n",
      "--------------------\n",
      "Input: delicate taste\n",
      "Output: < I' for the..\n",
      "M delicate, your taste.\n",
      "Ofet I' away.\n",
      "--------------------\n",
      "Evaluation took 10.5407s\n",
      "\n",
      "╒═══════════════════════╤════════════╤══════════════╤════════════╕\n",
      "│                       │      train │   validation │       test │\n",
      "╞═══════════════════════╪════════════╪══════════════╪════════════╡\n",
      "│ bleu                  │     0.0452 │       0.0714 │     0.0521 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ char_error_rate       │     0.6168 │       0.5544 │     0.5636 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ loss                  │     2.9427 │       2.3545 │     2.3968 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ next_token_perplexity │ 21597.9727 │   20048.1992 │ 20073.7676 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ perplexity            │ 31923.0996 │   31983.9844 │ 31987.0664 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_fmeasure       │     0.3398 │       0.3750 │     0.3647 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_precision      │     0.3702 │       0.3846 │     0.3782 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_recall         │     0.3188 │       0.3699 │     0.3556 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_fmeasure       │     0.0985 │       0.1446 │     0.1315 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_precision      │     0.1084 │       0.1487 │     0.1355 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_recall         │     0.0919 │       0.1430 │     0.1290 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_fmeasure       │     0.3080 │       0.3507 │     0.3400 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_precision      │     0.3355 │       0.3598 │     0.3523 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_recall         │     0.2891 │       0.3458 │     0.3318 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_fmeasure    │     0.3335 │       0.3727 │     0.3602 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_precision   │     0.3626 │       0.3823 │     0.3736 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_recall      │     0.3134 │       0.3675 │     0.3511 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ sequence_accuracy     │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ token_accuracy        │     0.0079 │       0.0043 │     0.0044 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ word_error_rate       │     0.8751 │       0.7875 │     0.8041 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ combined_loss         │     2.9427 │       2.3545 │     2.3968 │\n",
      "╘═══════════════════════╧════════════╧══════════════╧════════════╛\n",
      "Evaluation validation metric: 'text' 'loss' improved.\n",
      "'text' 'loss' decreased by inf.\n",
      "New best model saved.\n",
      "\n",
      "Training:  67%|██████▋   | 700/1050 [01:41<00:43,  8.04it/s, loss=0.272]\n",
      "Running evaluation for step: 700, epoch: 2\n",
      "Evaluation valid: 100%|██████████| 50/50 [00:03<00:00, 16.15it/s]\n",
      "Input: get downed\n",
      "Output: The Get do we?\n",
      "A get get to the.\n",
      "Get who get downed.\n",
      "--------------------\n",
      "Input: fight\n",
      "Output: < The give a fight.\n",
      "With someone old person..ll.\n",
      "Got nothing to lose.\n",
      "--------------------\n",
      "Input: nails\n",
      "Output: Winter N nails's nails.\n",
      "The'ill the\n",
      "The bag of nails.\n",
      "--------------------\n",
      "Input: Larry\n",
      "Output: <. <, Larry...\n",
      "Teaming and Larry Coll,.b.\n",
      "Martrownes, Larry N.\n",
      "--------------------\n",
      "Input: too many\n",
      "Output: I Moon have too many.\n",
      "Weories,...\n",
      "To calm up hell up.\n",
      "--------------------\n",
      "Evaluation test : 100%|██████████| 100/100 [00:06<00:00, 16.12it/s]\n",
      "Input: potpourri\n",
      "Output: < I' be..\n",
      "A a pot research of blo.\n",
      "With pot potpourri.\n",
      "--------------------\n",
      "Input: waves\n",
      "Output: I Wintering the waves.\n",
      "The manork cat.les.\n",
      "Ineneathide me.\n",
      "--------------------\n",
      "Input: nail\n",
      "Output: I Theairing\n",
      "The nail to.aceilles.\n",
      "Toail. the wall.\n",
      "--------------------\n",
      "Input: never been\n",
      "Output: < The' never been.\n",
      "Aed with a doctor one.\n",
      "I is love my life.\n",
      "--------------------\n",
      "Input: delicate taste\n",
      "Output: < The' for the..\n",
      "M delicate, delicate taste.\n",
      "Iet,' away.\n",
      "--------------------\n",
      "Evaluation took 9.5016s\n",
      "\n",
      "╒═══════════════════════╤════════════╤══════════════╤════════════╕\n",
      "│                       │      train │   validation │       test │\n",
      "╞═══════════════════════╪════════════╪══════════════╪════════════╡\n",
      "│ bleu                  │     0.0894 │       0.0896 │     0.0633 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ char_error_rate       │     0.5285 │       0.5396 │     0.5581 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ loss                  │     2.1325 │       2.2435 │     2.3237 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ next_token_perplexity │ 19270.9668 │   19539.4297 │ 19603.9258 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ perplexity            │ 31991.0020 │   31994.4199 │ 31993.7168 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_fmeasure       │     0.3726 │       0.3796 │     0.3622 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_precision      │     0.3824 │       0.3897 │     0.3735 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_recall         │     0.3665 │       0.3731 │     0.3550 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_fmeasure       │     0.1496 │       0.1557 │     0.1342 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_precision      │     0.1521 │       0.1606 │     0.1372 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_recall         │     0.1484 │       0.1525 │     0.1327 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_fmeasure       │     0.3530 │       0.3629 │     0.3394 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_precision      │     0.3620 │       0.3725 │     0.3499 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_recall         │     0.3474 │       0.3567 │     0.3329 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_fmeasure    │     0.3690 │       0.3787 │     0.3581 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_precision   │     0.3785 │       0.3888 │     0.3694 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_recall      │     0.3631 │       0.3722 │     0.3509 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ sequence_accuracy     │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ token_accuracy        │     0.0018 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ word_error_rate       │     0.7700 │       0.7701 │     0.7982 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ combined_loss         │     2.1325 │       2.2435 │     2.3237 │\n",
      "╘═══════════════════════╧════════════╧══════════════╧════════════╛\n",
      "Evaluation validation metric: 'text' 'loss' improved.\n",
      "'text' 'loss' decreased by 0.111.\n",
      "New best model saved.\n",
      "\n",
      "Training: 100%|██████████| 1050/1050 [02:35<00:00,  7.99it/s, loss=0.253]\n",
      "Running evaluation for step: 1050, epoch: 3\n",
      "Evaluation valid: 100%|██████████| 50/50 [00:03<00:00, 16.04it/s]\n",
      "Input: get downed\n",
      "Output: < Get do we?\n",
      "A get get to get.\n",
      "Get who get downed.\n",
      "--------------------\n",
      "Input: fight\n",
      "Output: < I give a fight.\n",
      "With a old person,.ll.\n",
      "Nothingot nothing to lose.\n",
      "--------------------\n",
      "Input: nails\n",
      "Output: I N father's hands.\n",
      "I hammerill nails\n",
      "The bag of nails.\n",
      "--------------------\n",
      "Input: Larry\n",
      "Output: <. <. a.ague.\n",
      "Teaming, David Coll,Jb.\n",
      "Dankses, Larry..\n",
      "--------------------\n",
      "Input: too many\n",
      "Output: I I' too many.\n",
      "Too same,...\n",
      "To calm the fuck up.\n",
      "--------------------\n",
      "Evaluation test : 100%|██████████| 100/100 [00:06<00:00, 16.10it/s]\n",
      "Input: potpourri\n",
      "Output: < I' be proud.\n",
      "About pot pot research, pot.\n",
      "P pot potpourri.\n",
      "--------------------\n",
      "Input: waves\n",
      "Output: I Wing the waves.\n",
      "And smallray cat.les.\n",
      "Ineneathide the.\n",
      "--------------------\n",
      "Input: nail\n",
      "Output: I Iairing\n",
      "Theo to.aceilles.\n",
      "Theail. the wall.\n",
      "--------------------\n",
      "Input: never been\n",
      "Output: < I' never been.\n",
      "Toed with a doctor one.\n",
      "I is love my life.\n",
      "--------------------\n",
      "Input: delicate taste\n",
      "Output: < I' for the..\n",
      "But delicate, delicate taste.\n",
      "Iet so' away.\n",
      "--------------------\n",
      "Evaluation took 9.5305s\n",
      "\n",
      "╒═══════════════════════╤════════════╤══════════════╤════════════╕\n",
      "│                       │      train │   validation │       test │\n",
      "╞═══════════════════════╪════════════╪══════════════╪════════════╡\n",
      "│ bleu                  │     0.0607 │       0.0648 │     0.0661 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ char_error_rate       │     0.4892 │       0.5477 │     0.5435 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ loss                  │     1.8102 │       2.2998 │     2.3723 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ next_token_perplexity │ 18493.2891 │   19400.4141 │ 19479.3574 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ perplexity            │ 31994.6309 │   31995.4844 │ 31990.6055 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_fmeasure       │     0.4365 │       0.3561 │     0.3622 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_precision      │     0.4382 │       0.3646 │     0.3751 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_recall         │     0.4385 │       0.3504 │     0.3533 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_fmeasure       │     0.2022 │       0.1287 │     0.1381 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_precision      │     0.2017 │       0.1319 │     0.1412 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_recall         │     0.2045 │       0.1266 │     0.1363 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_fmeasure       │     0.4226 │       0.3425 │     0.3413 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_precision      │     0.4238 │       0.3507 │     0.3534 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_recall         │     0.4250 │       0.3370 │     0.3331 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_fmeasure    │     0.4334 │       0.3541 │     0.3566 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_precision   │     0.4351 │       0.3627 │     0.3693 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_recall      │     0.4354 │       0.3485 │     0.3479 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ sequence_accuracy     │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ token_accuracy        │     0.0000 │       0.0000 │     0.0002 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ word_error_rate       │     0.7395 │       0.7995 │     0.7986 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ combined_loss         │     1.8102 │       2.2998 │     2.3723 │\n",
      "╘═══════════════════════╧════════════╧══════════════╧════════════╛\n",
      "Last improvement of text validation loss happened 350 step(s) ago.\n",
      "\n",
      "Training: 100%|██████████| 1050/1050 [02:45<00:00,  6.36it/s, loss=0.253]\n",
      "\n",
      "╒═════════════════╕\n",
      "│ TRAINING REPORT │\n",
      "╘═════════════════╛\n",
      "\n",
      "╒══════════════════════════════╤════════════════════╕\n",
      "│ Validation feature           │ text               │\n",
      "├──────────────────────────────┼────────────────────┤\n",
      "│ Validation metric            │ loss               │\n",
      "├──────────────────────────────┼────────────────────┤\n",
      "│ Best model step              │ 700                │\n",
      "├──────────────────────────────┼────────────────────┤\n",
      "│ Best model epoch             │ 3                  │\n",
      "├──────────────────────────────┼────────────────────┤\n",
      "│ Best model's validation loss │ 2.2434587478637695 │\n",
      "├──────────────────────────────┼────────────────────┤\n",
      "│ Best model's test loss       │ 2.323664903640747  │\n",
      "╘══════════════════════════════╧════════════════════╛\n",
      "\n",
      "Finished: api_experiment_run\n",
      "Saved to: /home/dave/code/llama-haiku/results/api_experiment_run_3\n",
      "\n",
      "╒══════════╕\n",
      "│ FINISHED │\n",
      "╘══════════╛\n",
      "\n",
      "contents of output directory: /home/dave/code/llama-haiku/results/api_experiment_run_3\n",
      "\t description.json\n",
      "\t model\n",
      "\t training_statistics.json\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import yaml\n",
    "\n",
    "from ludwig.api import LudwigModel\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<|system|>\n",
    "You are a haiku writer and respond to all questions with a colorful, poetic haiku</s>\n",
    "<|user|>\n",
    "Please write me a haiku about {keywords}</s>\n",
    "<|assistant|>\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# Build out the configuration\n",
    "config = yaml.safe_load(\n",
    "    \"\"\"\n",
    "model_type: llm\n",
    "base_model: HuggingFaceH4/zephyr-7b-beta\n",
    "\n",
    "quantization:\n",
    "  bits: 4\n",
    "\n",
    "adapter:\n",
    "  type: lora\n",
    "\n",
    "input_features:\n",
    "  - name: keywords\n",
    "    type: text\n",
    "\n",
    "output_features:\n",
    "  - name: text\n",
    "    type: text\n",
    "\n",
    "trainer:\n",
    "    type: finetune\n",
    "    learning_rate: 0.0003\n",
    "    batch_size: 2\n",
    "    gradient_accumulation_steps: 8\n",
    "    epochs: 3\n",
    "    learning_rate_scheduler:\n",
    "      warmup_fraction: 0.01\n",
    "\n",
    "backend:\n",
    "  type: local\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Define Ludwig model object that drive model training\n",
    "model = LudwigModel(config=config, logging_level=logging.INFO)\n",
    "\n",
    "# initiate model training\n",
    "(\n",
    "    train_stats,  # dictionary containing training statistics\n",
    "    preprocessed_data,  # tuple Ludwig Dataset objects of pre-processed training data\n",
    "    output_directory,  # location of training results stored on disk\n",
    ") = model.train(\n",
    "    dataset=haikus\n",
    ")\n",
    "\n",
    "# list contents of output directory\n",
    "print(\"contents of output directory:\", output_directory)\n",
    "for item in os.listdir(output_directory):\n",
    "    print(\"\\t\", item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, so how'd we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of HuggingFaceH4/zephyr-7b-beta tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 100%|██████████| 1/1 [00:11<00:00, 11.23s/it]\n",
      "Loaded HuggingFace implementation of HuggingFaceH4/zephyr-7b-beta tokenizer\n",
      "Finished predicting in: 13.18s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dave/miniconda3/envs/llm/lib/python3.9/site-packages/ludwig/features/feature_utils.py:102: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(np.log(sequence_probabilities))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                    text_predictions  \\\n",
       " 0  [, I, c, icles, ., \\n, The, sun, ., \\n, On, th...   \n",
       " 1  [, The, trees, are, ., \\n, So, tall, ,, I, can...   \n",
       " 2  [, I, ', m, walking, ., \\n, On, the, sidewalk,...   \n",
       " 3  [, I, ', m, glad, I, ', m, not, ., \\n, A, truc...   \n",
       " 4  [, I, ', m, not, a, data, ., \\n, S, ci, ence, ...   \n",
       " \n",
       "                                   text_probabilities  \\\n",
       " 0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       " 1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       " 2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       " 3  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       " 4  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       " \n",
       "                                        text_response  text_probability  \n",
       " 0               [Icicles.\\nThe sun.\\nOn the window.]              -inf  \n",
       " 1  [The trees are.\\nSo tall, I can't see.\\nThe to...              -inf  \n",
       " 2  [I'm walking.\\nOn the sidewalk and I see.\\nA f...              -inf  \n",
       " 3  [I'm glad I'm not.\\nA truck driver on the high...              -inf  \n",
       " 4  [I'm not a data.\\nScience guy, but I'm pretty....              -inf  ,\n",
       " 'results')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict({\n",
    "    \"keywords\": \n",
    "        [\n",
    "            \"icicles\",\n",
    "            \"trees\",\n",
    "            \"flowers on the sidewalk\", \n",
    "            \"trucks on the highway\", \n",
    "            \"data science gone wrong\"\n",
    "        ]\n",
    "    })\n",
    "\n",
    "response = model.predict(df)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Icicles.\n",
      "The sun.\n",
      "On the window.\n",
      "\n",
      "The trees are.\n",
      "So tall, I can't see.\n",
      "The top of them.\n",
      "\n",
      "I'm walking.\n",
      "On the sidewalk and I see.\n",
      "A flower on the ground.\n",
      "\n",
      "I'm glad I'm not.\n",
      "A truck driver on the highway.\n",
      "I'm glad I'm not.\n",
      "\n",
      "I'm not a data.\n",
      "Science guy, but I'm pretty.\n",
      "Sure this is going wrong.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "answers = response[0][\"text_response\"]\n",
    "\n",
    "for a in answers:\n",
    "    print(a[0] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, well, we'll leave it at that \n",
    "\n",
    "### Now we'll save the model to Hugging Face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model uploaded to `https://huggingface.co/querri/zephyr-haiku/tree/main/` with repository name `querri/zephyr-haiku`\n"
     ]
    }
   ],
   "source": [
    "!ludwig upload hf_hub -r querri/zephyr-haiku -m /home/dave/code/llama-haiku/results/api_experiment_run_0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
