{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train an LLM with custom data\n",
    "\n",
    "In this notebook we will prepare a training dataset and use it to train our custom model.\n",
    "\n",
    "You must have `HUGGINGFACE_API_KEY` in a `.env` file for this to work.\n",
    "\n",
    "First, we'll download a training dataset (only needs to run the first time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "dataset = \"statworx/haiku\"\n",
    "headers = {\"Authorization\": f\"Bearer {os.environ.get('HUGGINGFACE_API_KEY')}\"}\n",
    "API_URL = f\"https://datasets-server.huggingface.co/parquet?dataset={dataset}\"\n",
    "\n",
    "def query():\n",
    "    response = requests.get(API_URL, headers=headers)\n",
    "    return response.json()\n",
    "\n",
    "# get the url to the datafile\n",
    "data = query()\n",
    "url = data[\"parquet_files\"][0][\"url\"]\n",
    "\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "with open('data/haikus.parquet', 'wb') as file:\n",
    "    file.write(r.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data\n",
    "\n",
    "Next, we'll load the dataset into a pandas dataframe and prepare a training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>text</th>\n",
       "      <th>text_phonemes</th>\n",
       "      <th>keywords</th>\n",
       "      <th>keyword_phonemes</th>\n",
       "      <th>gruen_score</th>\n",
       "      <th>text_punc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>Delicate savage. / You'll never hold the cinde...</td>\n",
       "      <td>deh|lax|kaxt sae|vaxjh / yuwl neh|ver hhowld d...</td>\n",
       "      <td>cinder</td>\n",
       "      <td>sihn|der</td>\n",
       "      <td>0.639071</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>A splash and a cry. / Words pulled from the ri...</td>\n",
       "      <td>ax splaesh aend ax kray / werdz puhld frahm dh...</td>\n",
       "      <td>the riverside</td>\n",
       "      <td>dhax rih|ver|sayd</td>\n",
       "      <td>0.563353</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>Steamy, mist rising. / Rocks receiving downwar...</td>\n",
       "      <td>stiy|miy mihst ray|zaxng / raaks rax|siy|vaxng...</td>\n",
       "      <td>mist rising</td>\n",
       "      <td>mihst ray|zaxng</td>\n",
       "      <td>0.538326</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>You were broken glass. / But I touched you eve...</td>\n",
       "      <td>yuw wer brow|kaxn glaes / baht ay tahcht yuw i...</td>\n",
       "      <td>broken glass</td>\n",
       "      <td>brow|kaxn glaes</td>\n",
       "      <td>0.703446</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bfbarry</td>\n",
       "      <td>Eyes dance with firelight. / The Moon and I ar...</td>\n",
       "      <td>ayz daens wihdh faxr|layt / dhax muwn aend ay ...</td>\n",
       "      <td>eyes dance</td>\n",
       "      <td>ayz daens</td>\n",
       "      <td>0.830985</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49019</th>\n",
       "      <td>haiku_data_2</td>\n",
       "      <td>Alpine Lake. / Mybreaststrokesshiningarc. / To...</td>\n",
       "      <td>ael|payn leyk mih|brehst|strow|kehsh|hhax|nihn...</td>\n",
       "      <td>toward sunrise</td>\n",
       "      <td>tax|waord sahn|rayz</td>\n",
       "      <td>0.685355</td>\n",
       "      <td>Alpine Lake. Mybreaststrokesshiningarc. Toward...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49020</th>\n",
       "      <td>haiku_data_2</td>\n",
       "      <td>Spruce Woods. / Fireweed filling. / The vacancy.</td>\n",
       "      <td>spruws wuhdz fay|er|wiyd fih|laxng dhax vey|ka...</td>\n",
       "      <td>woods</td>\n",
       "      <td>wuhdz</td>\n",
       "      <td>0.568974</td>\n",
       "      <td>Spruce Woods. Fireweed filling. The vacancy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49021</th>\n",
       "      <td>haiku_data_2</td>\n",
       "      <td>Corrugated sun. / Chilies and laundry. / In ro...</td>\n",
       "      <td>kao|rax|gey|taxd sahn chih|liyz aend laon|driy...</td>\n",
       "      <td>sun chilies</td>\n",
       "      <td>sahn chih|liyz</td>\n",
       "      <td>0.551056</td>\n",
       "      <td>Corrugated sun. Chilies and laundry. In roofto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49022</th>\n",
       "      <td>haiku_data_2</td>\n",
       "      <td>Home from war. / We ease out. / The champagne ...</td>\n",
       "      <td>hhowm frahm waor wiy iyz awt dhax shaxm|peyn k...</td>\n",
       "      <td>home</td>\n",
       "      <td>hhowm</td>\n",
       "      <td>0.697112</td>\n",
       "      <td>Home from war. We ease out. The champagne corks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49023</th>\n",
       "      <td>haiku_data_2</td>\n",
       "      <td>'</td>\n",
       "      <td></td>\n",
       "      <td>N</td>\n",
       "      <td>ehn</td>\n",
       "      <td>0.625000</td>\n",
       "      <td>'</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>49024 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             source                                               text  \\\n",
       "0           bfbarry  Delicate savage. / You'll never hold the cinde...   \n",
       "1           bfbarry  A splash and a cry. / Words pulled from the ri...   \n",
       "2           bfbarry  Steamy, mist rising. / Rocks receiving downwar...   \n",
       "3           bfbarry  You were broken glass. / But I touched you eve...   \n",
       "4           bfbarry  Eyes dance with firelight. / The Moon and I ar...   \n",
       "...             ...                                                ...   \n",
       "49019  haiku_data_2  Alpine Lake. / Mybreaststrokesshiningarc. / To...   \n",
       "49020  haiku_data_2   Spruce Woods. / Fireweed filling. / The vacancy.   \n",
       "49021  haiku_data_2  Corrugated sun. / Chilies and laundry. / In ro...   \n",
       "49022  haiku_data_2  Home from war. / We ease out. / The champagne ...   \n",
       "49023  haiku_data_2                                                  '   \n",
       "\n",
       "                                           text_phonemes        keywords  \\\n",
       "0      deh|lax|kaxt sae|vaxjh / yuwl neh|ver hhowld d...          cinder   \n",
       "1      ax splaesh aend ax kray / werdz puhld frahm dh...   the riverside   \n",
       "2      stiy|miy mihst ray|zaxng / raaks rax|siy|vaxng...     mist rising   \n",
       "3      yuw wer brow|kaxn glaes / baht ay tahcht yuw i...    broken glass   \n",
       "4      ayz daens wihdh faxr|layt / dhax muwn aend ay ...      eyes dance   \n",
       "...                                                  ...             ...   \n",
       "49019  ael|payn leyk mih|brehst|strow|kehsh|hhax|nihn...  toward sunrise   \n",
       "49020  spruws wuhdz fay|er|wiyd fih|laxng dhax vey|ka...           woods   \n",
       "49021  kao|rax|gey|taxd sahn chih|liyz aend laon|driy...     sun chilies   \n",
       "49022  hhowm frahm waor wiy iyz awt dhax shaxm|peyn k...            home   \n",
       "49023                                                                  N   \n",
       "\n",
       "          keyword_phonemes  gruen_score  \\\n",
       "0                 sihn|der     0.639071   \n",
       "1        dhax rih|ver|sayd     0.563353   \n",
       "2          mihst ray|zaxng     0.538326   \n",
       "3          brow|kaxn glaes     0.703446   \n",
       "4                ayz daens     0.830985   \n",
       "...                    ...          ...   \n",
       "49019  tax|waord sahn|rayz     0.685355   \n",
       "49020                wuhdz     0.568974   \n",
       "49021       sahn chih|liyz     0.551056   \n",
       "49022                hhowm     0.697112   \n",
       "49023                  ehn     0.625000   \n",
       "\n",
       "                                               text_punc  \n",
       "0                                                   None  \n",
       "1                                                   None  \n",
       "2                                                   None  \n",
       "3                                                   None  \n",
       "4                                                   None  \n",
       "...                                                  ...  \n",
       "49019  Alpine Lake. Mybreaststrokesshiningarc. Toward...  \n",
       "49020       Spruce Woods. Fireweed filling. The vacancy.  \n",
       "49021  Corrugated sun. Chilies and laundry. In roofto...  \n",
       "49022   Home from war. We ease out. The champagne corks.  \n",
       "49023                                                  '  \n",
       "\n",
       "[49024 rows x 7 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "haikus = pd.read_parquet(\"data/haikus.parquet\")\n",
    "haikus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Haiku format:\n",
      "Starry night. / The queue for skates. / Moves slowly.\n",
      "\n",
      "With new-lines:\n",
      "Starry night.\n",
      "The queue for skates.\n",
      "Moves slowly.\n",
      "\n",
      "Snowed in.\n",
      "Fire wraps.\n",
      "Around a log.\n",
      "\n",
      "Last rays of sun.\n",
      "Crows suddenly.\n",
      "Goldwinged.\n",
      "\n",
      "Fireside.\n",
      "A piece of jigsaw.\n",
      "Slips into place.\n",
      "\n",
      "We may start seeing.\n",
      "A lot of those walking boots.\n",
      "On people soon, lol.\n"
     ]
    }
   ],
   "source": [
    "# Let's take just a random sample of 5000 of these.\n",
    "haikus_text = haikus[\"text\"].sample(100)\n",
    "\n",
    "# print the first haiku\n",
    "print(\"Original Haiku format:\")\n",
    "print(haikus_text.iloc[0])\n",
    "\n",
    "# Add newlines\n",
    "haikus_text = haikus_text.str.replace(\" / \", \"\\n\")\n",
    "print(\"\\nWith new-lines:\")\n",
    "print(haikus_text.iloc[0])\n",
    "\n",
    "# Look at 4 more of them\n",
    "for i in range(1, 5):\n",
    "    print(\"\\n\" + haikus_text.iloc[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This time we'll keep them all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delicate savage.\n",
      "You'll never hold the cinder.\n",
      "But still you will burn.\n",
      "\n",
      "A splash and a cry.\n",
      "Words pulled from the riverside.\n",
      "Dryed in the hot sun.\n",
      "\n",
      "Steamy, mist rising.\n",
      "Rocks receiving downward crash.\n",
      "As the jungle weeps.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# See if we can get some consistent formatting\n",
    "haikus[\"text\"] = haikus[\"text\"].str.replace(\" / \", \"\\n\")\n",
    "\n",
    "for i in range(3):\n",
    "    print(haikus.iloc[i][\"text\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Good enough, let's train!\n",
    "\n",
    "Next up we'll use the \"text\" and \"keywords\" columns to construct a training dataset to use for training our model with this filtered set of haikus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version 2.1.2 available.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dave/miniconda3/envs/llm/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting generation max_new_tokens to 16384 to correspond with the max sequence length assigned to the output feature or the global max sequence length. This will ensure that the correct number of tokens are generated at inference time. To override this behavior, set `generation.max_new_tokens` to a different value in your Ludwig config.\n",
      "\n",
      "╒════════════════════════╕\n",
      "│ EXPERIMENT DESCRIPTION │\n",
      "╘════════════════════════╛\n",
      "\n",
      "╒══════════════════╤══════════════════════════════════════════════════════════════════════════════════════╕\n",
      "│ Experiment name  │ api_experiment                                                                       │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Model name       │ run                                                                                  │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ Output directory │ /home/dave/code/llama-haiku/results/api_experiment_run_0                             │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ ludwig_version   │ '0.9.1'                                                                              │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ command          │ ('/home/dave/miniconda3/envs/llm/lib/python3.9/site-packages/ipykernel_launcher.py ' │\n",
      "│                  │  '--f=/home/dave/.local/share/jupyter/runtime/kernel-v2-891279oMU7GT8cl6e.json')     │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ commit_hash      │ '7dd22286d632'                                                                       │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ random_seed      │ 42                                                                                   │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ data_format      │ \"<class 'pandas.core.frame.DataFrame'>\"                                              │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ torch_version    │ '2.1.2+cu121'                                                                        │\n",
      "├──────────────────┼──────────────────────────────────────────────────────────────────────────────────────┤\n",
      "│ compute          │ {   'arch_list': [   'sm_50',                                                        │\n",
      "│                  │                      'sm_60',                                                        │\n",
      "│                  │                      'sm_70',                                                        │\n",
      "│                  │                      'sm_75',                                                        │\n",
      "│                  │                      'sm_80',                                                        │\n",
      "│                  │                      'sm_86',                                                        │\n",
      "│                  │                      'sm_90'],                                                       │\n",
      "│                  │     'devices': {   0: {   'device_capability': (8, 9),                               │\n",
      "│                  │                           'device_properties': \"_CudaDeviceProperties(name='NVIDIA \" │\n",
      "│                  │                                                \"GeForce RTX 4090', major=8, \"        │\n",
      "│                  │                                                'minor=9, total_memory=24563MB, '     │\n",
      "│                  │                                                'multi_processor_count=128)',         │\n",
      "│                  │                           'gpu_type': 'NVIDIA GeForce RTX 4090'}},                   │\n",
      "│                  │     'gencode_flags': '-gencode compute=compute_50,code=sm_50 -gencode '              │\n",
      "│                  │                      'compute=compute_60,code=sm_60 -gencode '                       │\n",
      "│                  │                      'compute=compute_70,code=sm_70 -gencode '                       │\n",
      "│                  │                      'compute=compute_75,code=sm_75 -gencode '                       │\n",
      "│                  │                      'compute=compute_80,code=sm_80 -gencode '                       │\n",
      "│                  │                      'compute=compute_86,code=sm_86 -gencode '                       │\n",
      "│                  │                      'compute=compute_90,code=sm_90',                                │\n",
      "│                  │     'gpus_per_node': 1,                                                              │\n",
      "│                  │     'num_nodes': 1}                                                                  │\n",
      "╘══════════════════╧══════════════════════════════════════════════════════════════════════════════════════╛\n",
      "\n",
      "╒═══════════════╕\n",
      "│ LUDWIG CONFIG │\n",
      "╘═══════════════╛\n",
      "\n",
      "User-specified config (with upgrades):\n",
      "\n",
      "{   'adapter': {'type': 'lora'},\n",
      "    'backend': {'type': 'local'},\n",
      "    'base_model': 'HuggingFaceH4/zephyr-7b-beta',\n",
      "    'input_features': [{'name': 'keywords', 'type': 'text'}],\n",
      "    'ludwig_version': '0.9.1',\n",
      "    'model_type': 'llm',\n",
      "    'output_features': [{'name': 'text', 'type': 'text'}],\n",
      "    'quantization': {'bits': 4},\n",
      "    'trainer': {   'batch_size': 2,\n",
      "                   'epochs': 3,\n",
      "                   'gradient_accumulation_steps': 8,\n",
      "                   'learning_rate': 0.0003,\n",
      "                   'learning_rate_scheduler': {'warmup_fraction': 0.01},\n",
      "                   'type': 'finetune'}}\n",
      "\n",
      "Full config saved to:\n",
      "/home/dave/code/llama-haiku/results/api_experiment_run_0/api_experiment/model/model_hyperparameters.json\n",
      "\n",
      "╒═══════════════╕\n",
      "│ PREPROCESSING │\n",
      "╘═══════════════╛\n",
      "\n",
      "No cached dataset found at /home/dave/code/llama-haiku/d9b50b24af4c11eea87be55baf3cf3ab.training.hdf5. Preprocessing the dataset.\n",
      "Using full dataframe\n",
      "Building dataset (it may take a while)\n",
      "Loaded HuggingFace implementation of HuggingFaceH4/zephyr-7b-beta tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of feature 'keywords': 21 (without start and stop symbols)\n",
      "Max sequence length is 21 for feature 'keywords'\n",
      "Loaded HuggingFace implementation of HuggingFaceH4/zephyr-7b-beta tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of feature 'text': 46 (without start and stop symbols)\n",
      "Max sequence length is 46 for feature 'text'\n",
      "Loaded HuggingFace implementation of HuggingFaceH4/zephyr-7b-beta tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of HuggingFaceH4/zephyr-7b-beta tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataset: DONE\n",
      "Writing preprocessed training set cache to /home/dave/code/llama-haiku/d9b50b24af4c11eea87be55baf3cf3ab.training.hdf5\n",
      "Writing preprocessed validation set cache to /home/dave/code/llama-haiku/d9b50b24af4c11eea87be55baf3cf3ab.validation.hdf5\n",
      "Writing preprocessed test set cache to /home/dave/code/llama-haiku/d9b50b24af4c11eea87be55baf3cf3ab.test.hdf5\n",
      "Writing train set metadata to /home/dave/code/llama-haiku/d9b50b24af4c11eea87be55baf3cf3ab.meta.json\n",
      "\n",
      "Dataset Statistics\n",
      "╒════════════╤═══════════════╤════════════════════╕\n",
      "│ Dataset    │   Size (Rows) │ Size (In Memory)   │\n",
      "╞════════════╪═══════════════╪════════════════════╡\n",
      "│ Training   │         34317 │ 7.85 Mb            │\n",
      "├────────────┼───────────────┼────────────────────┤\n",
      "│ Validation │          4902 │ 1.12 Mb            │\n",
      "├────────────┼───────────────┼────────────────────┤\n",
      "│ Test       │          9805 │ 2.24 Mb            │\n",
      "╘════════════╧═══════════════╧════════════════════╛\n",
      "\n",
      "╒═══════╕\n",
      "│ MODEL │\n",
      "╘═══════╛\n",
      "\n",
      "Warnings and other logs:\n",
      "Loading large language model...\n",
      "We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:06<00:00,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done.\n",
      "Loaded HuggingFace implementation of HuggingFaceH4/zephyr-7b-beta tokenizer\n",
      "==================================================\n",
      "Trainable Parameter Summary For Fine-Tuning\n",
      "Fine-tuning with adapter: lora\n",
      "trainable params: 3,407,872 || all params: 7,245,139,968 || trainable%: 0.04703666202518836\n",
      "==================================================\n",
      "\n",
      "╒══════════╕\n",
      "│ TRAINING │\n",
      "╘══════════╛\n",
      "\n",
      "Creating fresh model training run.\n",
      "Training for 51477 step(s), approximately 3 epoch(s).\n",
      "Early stopping policy: 5 round(s) of evaluation, or 85795 step(s), approximately 5 epoch(s).\n",
      "\n",
      "Starting with step 0, epoch: 0\n",
      "Training:  33%|███▎      | 17158/51477 [39:14<1:17:05,  7.42it/s, loss=0.205]Last batch in epoch only has 1 sample and will be dropped.\n",
      "Last batch in epoch only has 1 sample and will be dropped.\n",
      "Last batch in epoch only has 1 sample and will be dropped.\n",
      "Training:  33%|███▎      | 17159/51477 [39:15<1:37:00,  5.90it/s, loss=0.225]\n",
      "Running evaluation for step: 17159, epoch: 1\n",
      "Evaluation valid: 100%|██████████| 2451/2451 [03:08<00:00, 13.00it/s]\n",
      "Input: room different\n",
      "Output: < I' in in.\n",
      "A room different from the.\n",
      "One I left in..\n",
      "--------------------\n",
      "Input: icicles still\n",
      "Output: <eland I, of spring.\n",
      "Inowgeop icicles.\n",
      "Still hanging from the\n",
      "--------------------\n",
      "Input: bark\n",
      "Output: < Thech bark.\n",
      "Thepl.. the wind.\n",
      "The..\n",
      "--------------------\n",
      "Input: nail\n",
      "Output: < I nail ininking..\n",
      "The heart's hand.\n",
      "The the hand.\n",
      "--------------------\n",
      "Input: busy highway\n",
      "Output: < Busched on a fencerail.\n",
      "A a busy highway.\n",
      "Aningret.ens.\n",
      "--------------------\n",
      "Evaluation test : 100%|██████████| 4903/4903 [07:18<00:00, 11.19it/s]\n",
      "Input: some flowers\n",
      "Output: < I' I'.\n",
      "Get, something some over by.\n",
      "Getek some flowers..\n",
      "--------------------\n",
      "Input: reminding\n",
      "Output: < Iaring, I,.\n",
      "L, I for reminding me\n",
      "Me to our mistake.\n",
      "--------------------\n",
      "Input: respect them\n",
      "Output: < I, I's not.\n",
      "Aboutpect to I you not.\n",
      "You you respect them.\n",
      "--------------------\n",
      "Input: of sorrows\n",
      "Output: <row The are escape..\n",
      "P of sorrows, but we can.\n",
      "Coose to be without love.\n",
      "--------------------\n",
      "Input: want respect\n",
      "Output: < I want want respect.\n",
      "And can't gotta like me, but.\n",
      "You gotta respect me.\n",
      "--------------------\n",
      "Evaluation took 10m 30.9988s\n",
      "\n",
      "╒═══════════════════════╤════════════╤══════════════╤════════════╕\n",
      "│                       │      train │   validation │       test │\n",
      "╞═══════════════════════╪════════════╪══════════════╪════════════╡\n",
      "│ bleu                  │     0.0000 │       0.0862 │     0.0854 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ char_error_rate       │     0.4191 │       0.5163 │     0.5126 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ loss                  │     1.7968 │       1.9949 │     1.9921 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ next_token_perplexity │ 18025.0820 │   18861.8770 │ 18839.6738 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ perplexity            │ 31977.0918 │   31985.1133 │ 31975.3535 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_fmeasure       │     0.4569 │       0.4081 │     0.4081 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_precision      │     0.4500 │       0.4180 │     0.4179 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_recall         │     0.4643 │       0.4010 │     0.4010 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_fmeasure       │     0.1650 │       0.1708 │     0.1710 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_precision      │     0.1623 │       0.1748 │     0.1751 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_recall         │     0.1678 │       0.1680 │     0.1681 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_fmeasure       │     0.4152 │       0.3923 │     0.3927 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_precision      │     0.4083 │       0.4017 │     0.4020 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_recall         │     0.4226 │       0.3856 │     0.3859 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_fmeasure    │     0.4569 │       0.4053 │     0.4053 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_precision   │     0.4500 │       0.4150 │     0.4150 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_recall      │     0.4643 │       0.3982 │     0.3982 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ sequence_accuracy     │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ token_accuracy        │     0.0000 │       0.0002 │     0.0001 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ word_error_rate       │     0.7500 │       0.7569 │     0.7528 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ combined_loss         │     1.7968 │       1.9949 │     1.9921 │\n",
      "╘═══════════════════════╧════════════╧══════════════╧════════════╛\n",
      "Evaluation validation metric: 'text' 'loss' improved.\n",
      "'text' 'loss' decreased by inf.\n",
      "New best model saved.\n",
      "\n",
      "Training:  67%|██████▋   | 34316/51477 [1:28:10<39:18,  7.28it/s, loss=0.251]    Last batch in epoch only has 1 sample and will be dropped.\n",
      "Last batch in epoch only has 1 sample and will be dropped.\n",
      "Last batch in epoch only has 1 sample and will be dropped.\n",
      "Training:  67%|██████▋   | 34318/51477 [1:28:11<45:41,  6.26it/s, loss=0.182]\n",
      "Running evaluation for step: 34318, epoch: 2\n",
      "Evaluation valid: 100%|██████████| 2451/2451 [03:05<00:00, 13.21it/s]\n",
      "Input: room different\n",
      "Output: < I' so in.\n",
      "A room different from the.\n",
      "One I left in now.\n",
      "--------------------\n",
      "Input: icicles still\n",
      "Output: < The snow of spring.\n",
      "Thenowgeed icicles.\n",
      "Still hanging. the\n",
      "--------------------\n",
      "Input: bark\n",
      "Output: < Thech bark.\n",
      "Theappeed. the wind.\n",
      "A..\n",
      "--------------------\n",
      "Input: nail\n",
      "Output: < I nail inings..\n",
      "The heart's words.\n",
      "The my heart.\n",
      "--------------------\n",
      "Input: busy highway\n",
      "Output: < Busched on a fencerail.\n",
      "A a busy highway.\n",
      "Aningret.pa.\n",
      "--------------------\n",
      "Evaluation test : 100%|██████████| 4903/4903 [07:20<00:00, 11.12it/s]\n",
      "Input: some flowers\n",
      "Output: < I' I'.\n",
      "Get some something some over by.\n",
      "Getek some flowers.\n",
      "--------------------\n",
      "Input: reminding\n",
      "Output: < Remaring, I,.\n",
      "L, I for reminding.\n",
      "Me, that age.\n",
      "--------------------\n",
      "Input: respect them\n",
      "Output: < I, I's so.\n",
      "Aboutpect, love you not.\n",
      "You you respect them.\n",
      "--------------------\n",
      "Input: of sorrows\n",
      "Output: < The are help all.\n",
      "P of sorrows, but we can.\n",
      "Coose to bear in it.\n",
      "--------------------\n",
      "Input: want respect\n",
      "Output: < I want want respect.\n",
      "From don't gotta like me, I.\n",
      "I gotta not me.\n",
      "--------------------\n",
      "Evaluation took 10m 30.6246s\n",
      "\n",
      "╒═══════════════════════╤════════════╤══════════════╤════════════╕\n",
      "│                       │      train │   validation │       test │\n",
      "╞═══════════════════════╪════════════╪══════════════╪════════════╡\n",
      "│ bleu                  │     0.3457 │       0.0882 │     0.0859 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ char_error_rate       │     0.3238 │       0.5147 │     0.5121 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ loss                  │     1.4587 │       1.9746 │     1.9696 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ next_token_perplexity │ 17628.2324 │   18681.5098 │ 18656.8945 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ perplexity            │ 31989.6914 │   31978.9199 │ 31969.5918 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_fmeasure       │     0.5625 │       0.4150 │     0.4132 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_precision      │     0.5625 │       0.4295 │     0.4274 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge1_recall         │     0.5625 │       0.4037 │     0.4022 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_fmeasure       │     0.2143 │       0.1745 │     0.1748 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_precision      │     0.2143 │       0.1806 │     0.1809 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rouge2_recall         │     0.2143 │       0.1698 │     0.1702 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_fmeasure       │     0.5625 │       0.3980 │     0.3976 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_precision      │     0.5625 │       0.4117 │     0.4112 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeL_recall         │     0.5625 │       0.3873 │     0.3871 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_fmeasure    │     0.5625 │       0.4116 │     0.4106 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_precision   │     0.5625 │       0.4259 │     0.4247 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ rougeLsum_recall      │     0.5625 │       0.4004 │     0.3996 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ sequence_accuracy     │     0.0000 │       0.0000 │     0.0000 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ token_accuracy        │     0.0000 │       0.0001 │     0.0001 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ word_error_rate       │     0.6000 │       0.7503 │     0.7477 │\n",
      "├───────────────────────┼────────────┼──────────────┼────────────┤\n",
      "│ combined_loss         │     1.4587 │       1.9746 │     1.9696 │\n",
      "╘═══════════════════════╧════════════╧══════════════╧════════════╛\n",
      "Evaluation validation metric: 'text' 'loss' improved.\n",
      "'text' 'loss' decreased by 0.02.\n",
      "New best model saved.\n",
      "\n",
      "Training:  95%|█████████▌| 49096/51477 [2:11:18<05:13,  7.59it/s, loss=0.185]     "
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import yaml\n",
    "\n",
    "from ludwig.api import LudwigModel\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "<|system|>\n",
    "You are a haiku writer and respond to all questions with a colorful, poetic haiku</s>\n",
    "<|user|>\n",
    "Please write me a haiku about {keywords}</s>\n",
    "<|assistant|>\n",
    "{text}\n",
    "\"\"\"\n",
    "\n",
    "# Build out the configuration\n",
    "config = yaml.safe_load(\n",
    "    \"\"\"\n",
    "model_type: llm\n",
    "base_model: HuggingFaceH4/zephyr-7b-beta\n",
    "\n",
    "quantization:\n",
    "  bits: 4\n",
    "\n",
    "adapter:\n",
    "  type: lora\n",
    "\n",
    "input_features:\n",
    "  - name: keywords\n",
    "    type: text\n",
    "\n",
    "output_features:\n",
    "  - name: text\n",
    "    type: text\n",
    "\n",
    "trainer:\n",
    "    type: finetune\n",
    "    learning_rate: 0.0003\n",
    "    batch_size: 2\n",
    "    gradient_accumulation_steps: 8\n",
    "    epochs: 3\n",
    "    learning_rate_scheduler:\n",
    "      warmup_fraction: 0.01\n",
    "\n",
    "backend:\n",
    "  type: local\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "# Define Ludwig model object that drive model training\n",
    "model = LudwigModel(config=config, logging_level=logging.INFO)\n",
    "\n",
    "# initiate model training\n",
    "(\n",
    "    train_stats,  # dictionary containing training statistics\n",
    "    preprocessed_data,  # tuple Ludwig Dataset objects of pre-processed training data\n",
    "    output_directory,  # location of training results stored on disk\n",
    ") = model.train(\n",
    "    dataset=valid_haikus\n",
    ")\n",
    "\n",
    "# list contents of output directory\n",
    "print(\"contents of output directory:\", output_directory)\n",
    "for item in os.listdir(output_directory):\n",
    "    print(\"\\t\", item)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ok, so how'd we do?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded HuggingFace implementation of HuggingFaceH4/zephyr-7b-beta tokenizer\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 100%|██████████| 1/1 [00:06<00:00,  6.04s/it]\n",
      "Loaded HuggingFace implementation of HuggingFaceH4/zephyr-7b-beta tokenizer\n",
      "Finished predicting in: 7.68s.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dave/miniconda3/envs/llm/lib/python3.9/site-packages/ludwig/features/feature_utils.py:102: RuntimeWarning: divide by zero encountered in log\n",
      "  return np.sum(np.log(sequence_probabilities))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(                                    text_predictions  \\\n",
       " 0  [, I, ', m, not, a, flower, ., /, I, ', m, not...   \n",
       " 1  [, I, ', m, not, gonna, lie, ., /, I, ', m, re...   \n",
       " 2  [, I, ', m, really, good, at, ., /, Data, anal...   \n",
       " \n",
       "                                   text_probabilities  \\\n",
       " 0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       " 1  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       " 2  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ...   \n",
       " \n",
       "                                        text_response  text_probability  \n",
       " 0  [I'm not a flower. / I'm not a flower, I'm not...              -inf  \n",
       " 1  [I'm not gonna lie. / I'm really looking forwa...              -inf  \n",
       " 2  [I'm really good at. / Data analysis, but I'm....              -inf  ,\n",
       " 'results')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame.from_dict({\n",
    "    \"keywords\": \n",
    "        [\n",
    "            \"icicles\",\n",
    "            \"trees\",\n",
    "            \"flowers on the sidewalk\", \n",
    "            \"trucks on the highway\", \n",
    "            \"data science gone wrong\"\n",
    "        ]\n",
    "    })\n",
    "\n",
    "response = model.predict(df)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm not a flower. / I'm not a flower, I'm not. / A flower, I'm not.\n",
      "I'm not gonna lie. / I'm really looking forward. / To the trucks tonight.\n",
      "I'm really good at. / Data analysis, but I'm. / Not good at math.\n"
     ]
    }
   ],
   "source": [
    "answers = response[0][\"text_response\"]\n",
    "\n",
    "for a in answers:\n",
    "    print(a[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
